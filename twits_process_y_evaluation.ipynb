{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anticipated-amplifier",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "taken-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from gsitk.preprocess import pprocess_twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-wales",
   "metadata": {},
   "source": [
    "Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "american-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this file we take the tweets from the database of hate_labeled_data.csv and put in a file, with each line a tweet\n",
    "data = pd.read_csv('hate_labeled_data.csv')\n",
    "data = data.rename(columns={'count': 'counter', 'class': 'type'})   # change names so they dont interfere with class names\n",
    "data = data.sort_values(by='counter', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "banner-color",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>type</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14007</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>rt &lt;allcaps&gt; &lt;user&gt;: &lt;user&gt; &lt;user&gt; nah boa tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7415</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a bitch like you need to get fucked right ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13370</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>nikko a gay bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11155</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>i'd rather follow some girls on instagram rath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20709</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>she a hoe cause your boyfriend want her? she s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8516</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>confident in one way, pussy in another. so str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8514</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>completely off topic, idgaf about that pretty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>commentators bitch at bird for \"sliding\" but &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8512</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>comin from a fat bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>~~ruffled | ntac eileen dahlia - beautiful col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       counter  hate_speech  offensive_language  neither  type  \\\n",
       "14007        9            1                   8        0     1   \n",
       "7415         9            2                   7        0     1   \n",
       "13370        9            2                   7        0     1   \n",
       "11155        9            0                   3        6     2   \n",
       "20709        9            1                   7        1     1   \n",
       "...        ...          ...                 ...      ...   ...   \n",
       "8516         3            0                   3        0     1   \n",
       "8514         3            0                   3        0     1   \n",
       "8513         3            0                   3        0     1   \n",
       "8512         3            0                   3        0     1   \n",
       "24782        3            0                   0        3     2   \n",
       "\n",
       "                                                   tweet  \n",
       "14007  rt <allcaps> <user>: <user> <user> nah boa tha...  \n",
       "7415   a bitch like you need to get fucked right ever...  \n",
       "13370                                  nikko a gay bitch  \n",
       "11155  i'd rather follow some girls on instagram rath...  \n",
       "20709  she a hoe cause your boyfriend want her? she s...  \n",
       "...                                                  ...  \n",
       "8516   confident in one way, pussy in another. so str...  \n",
       "8514   completely off topic, idgaf about that pretty ...  \n",
       "8513   commentators bitch at bird for \"sliding\" but <...  \n",
       "8512                              comin from a fat bitch  \n",
       "24782  ~~ruffled | ntac eileen dahlia - beautiful col...  \n",
       "\n",
       "[24783 rows x 6 columns]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-chaos",
   "metadata": {},
   "source": [
    "Preprocess the twits with gsitk tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "surgical-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].map(pprocess_twitter.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "prescribed-budapest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hate twits (1430, 3)\n",
      "Offensive twits (19190, 3)\n",
      "Neither twits (4163, 3)\n"
     ]
    }
   ],
   "source": [
    "# divide the dataset into the category it belongs\n",
    "# we are going to make two tests, one with offensive language and one with hate_speech\n",
    "\n",
    "# separate datasets into neither, offensive and hate speech\n",
    "data_hate = data[data.type == 0][['counter', 'type', 'tweet']]\n",
    "data_offensive = data[data.type == 1][['counter', 'type', 'tweet']]\n",
    "data_neither = data[data.type == 2][['counter', 'type', 'tweet']]\n",
    "\n",
    "print('Hate twits', data_hate.shape)    # ->1430\n",
    "print('Offensive twits', data_offensive.shape)  # -> 19190\n",
    "print('Neither twits', data_neither.shape)  # -> 4163\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-schedule",
   "metadata": {},
   "source": [
    "using stratify to use all the information and maintain the proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "recent-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hate = data_hate.append(data_neither)\n",
    "dataset_hate_label = dataset_hate['type']\n",
    "train_hate, test_hate, train_hate_label, test_hate_label = train_test_split(\n",
    "    dataset_hate['tweet'], dataset_hate_label, test_size = 0.2, shuffle = True, stratify=dataset_hate_label)\n",
    "\n",
    "dataset_off = data_offensive.append(data_neither)\n",
    "dataset_off_label = dataset_off['type']\n",
    "train_off, test_off, train_off_label, test_off_label = train_test_split(dataset_off['tweet'], dataset_off_label,\n",
    "                                                                      test_size = 0.2, shuffle=True, stratify=dataset_off_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "valued-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_hate = test_hate.to_frame()\n",
    "test_hate['label'] = test_hate_label\n",
    "train_hate.keys()\n",
    "\n",
    "test_off = test_off.to_frame()\n",
    "test_off['label'] = test_off_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "respiratory-affairs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>\"&lt;user&gt;: &amp;&lt;hastag&gt; &lt;number&gt;;&lt;user&gt;: y'all thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7146</th>\n",
       "      <td>&lt;user&gt; i'd give that hoe the finger everyday a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8650</th>\n",
       "      <td>dat bitch gotta lay on her back to pee &lt;url&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9846</th>\n",
       "      <td>hoes always blaming hoes for being hoes.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15912</th>\n",
       "      <td>rt &lt;allcaps&gt; &lt;user&gt;: my bitch better lie to me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15004</th>\n",
       "      <td>rt &lt;allcaps&gt; &lt;user&gt; if k michelle was apart of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16700</th>\n",
       "      <td>rt &lt;allcaps&gt; &lt;user&gt;: &amp;&lt;hastag&gt; &lt;number&gt;;&lt;user&gt;...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>&amp;&lt;hastag&gt; &lt;number&gt;;&amp;#&lt;number&gt;;&amp;#&lt;number&gt;;&amp;#&lt;nu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4579</th>\n",
       "      <td>&lt;user&gt; so says the one who fills an empty hole...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>&amp;&lt;hastag&gt; &lt;number&gt;;&lt;user&gt;: when she ask how im...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4671 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  label\n",
       "390    \"<user>: &<hastag> <number>;<user>: y'all thin...      1\n",
       "7146   <user> i'd give that hoe the finger everyday a...      1\n",
       "8650        dat bitch gotta lay on her back to pee <url>      1\n",
       "9846            hoes always blaming hoes for being hoes.      1\n",
       "15912  rt <allcaps> <user>: my bitch better lie to me...      1\n",
       "...                                                  ...    ...\n",
       "15004  rt <allcaps> <user> if k michelle was apart of...      1\n",
       "16700  rt <allcaps> <user>: &<hastag> <number>;<user>...      1\n",
       "1058   &<hastag> <number>;&#<number>;&#<number>;&#<nu...      1\n",
       "4579   <user> so says the one who fills an empty hole...      1\n",
       "1504   &<hastag> <number>;<user>: when she ask how im...      1\n",
       "\n",
       "[4671 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # as we have dispair number of tweets for each class, and we want the data to be balanced\n",
    "# # we will make the two datasets with the minimum of the values as the limiting one\n",
    "# # concatenate both dataframes into a final one\n",
    "# dataset_offensive = data_offensive[0:5000].append(data_neither)\n",
    "# dataset_hate = data_hate.append(data_neither[0:2000])\n",
    "# # we shuffle it so the order is random and twits offensive and neither are mixed\n",
    "# dataset_offensive = shuffle(dataset_offensive)\n",
    "# dataset_hate = shuffle(dataset_hate)\n",
    "# # divide into train/test sets\n",
    "# train_offensive, test_offensive = train_test_split(dataset_offensive, test_size=0.2)\n",
    "# train_hate, test_hate = train_test_split(dataset_hate, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-modem",
   "metadata": {},
   "source": [
    "Save train files as txt and test files as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "latin-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create or erase content of file in case it is exists\n",
    "f = open('offensive_twits_train.txt', 'w+')\n",
    "for twit in train_off['tweet']:\n",
    "    f.write(twit + '\\n')\n",
    "f.close()\n",
    "\n",
    "f = open('hate_twits_train.txt', 'w+')\n",
    "for twit in train_hate['tweet']:\n",
    "    f.write(twit + '\\n')\n",
    "f.close()\n",
    "\n",
    "# save test files as csv\n",
    "test_off.to_csv('offensive_twits_test.csv')\n",
    "test_hate.to_csv('hate_twits_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-ivory",
   "metadata": {},
   "source": [
    "###### Execute 'hatedictionary.py hate_twits_train.txt hate' before continuing with the next part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-instruction",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "steady-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-adelaide",
   "metadata": {},
   "source": [
    "get the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "gross-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hate\n",
    "with open('dictionaries/hate_densify.json') as json_file:\n",
    "    hate_dict_densify = json.load(json_file)\n",
    "with open('dictionaries/hate_socialsent.json') as json_file:\n",
    "    hate_dict_socialsent = json.load(json_file)\n",
    "    \n",
    "# offensive\n",
    "with open('dictionaries/offensive_densify.json') as json_file:\n",
    "    offensive_dict_densify = json.load(json_file)\n",
    "with open('dictionaries/offensive_socialsent.json') as json_file:\n",
    "    offensive_dict_socialsent = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "competitive-message",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15106"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hate_dict_socialsent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "familiar-weather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.314330577850342"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(hate_dict_densify.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "sorted-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys_socialsent = sorted(hate_dict_socialsent, key=hate_dict_socialsent.get)\n",
    "sorted_keys_densify= sorted(hate_dict_densify, key=hate_dict_densify.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "compliant-gardening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values: \n",
      " ['nigger', 'motherfucker', 'faggot', 'asshole', 'fucker', 'cunt', 'dickhead', 'fag', 'bastard', 'skank', 'whore', 'douchebag', 'twat', 'chav', 'moron', 'jerk', 'dork', 'cocksucker', 'faggots', 'fucka', 'tramp', 'cunts', 'prick', 'licker', 'fags', 'mothafucka', 'wetback', 'loser', 'niggers', 'assholes', 'mudda', 'slut', 'queer', 'muthafucka', 'motha', 'turd', 'fuckers', 'twats', 'motherfuckers', 'bastards', 'dyke', 'mothafucker', 'dickheads', 'jerks', 'whores', 'dumbass', 'douche', 'scumbags', 'pussies', 'suckers']\n",
      "Positive values: \n",
      " ['enjoy', 'unreal', 'amazin', 'loved', 'epic', 'addition', 'fun', 'performance', 'impeccable', 'favorite', 'tremendous', 'talented', 'unbelievable', 'perfection', 'beauty', 'inspirational', 'nice', 'special', 'flawless', 'iconic', 'exceptional', 'atmosphere', 'magical', 'excellent', 'inspired', 'favourite', 'beautiful', 'gorgeous', 'inspiration', 'perfect', 'impressive', 'fab', 'exciting', 'inspiring', 'marvelous', 'great', 'best', 'remarkable', 'brilliant', 'lovely', 'outstanding', 'incredible', 'phenomenal', 'superb', 'stunning', 'fabulous', 'wonderful', 'awesome', 'amazing', 'fantastic']\n"
     ]
    }
   ],
   "source": [
    "print('Negative values: \\n', sorted_keys_socialsent[0:50])\n",
    "print('Positive values: \\n', sorted_keys_socialsent[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "instructional-clearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values: \n",
      " ['hoeass', 'faggit', 'fuckboy', 'wetback', 'bitchnigga', 'custy', 'biiiiiiiitch', 'spic', 'brolic', 'biiiiitch', 'wigger', 'dumass', 'shutcho', 'wicha', 'bitch-ass', 'greaseball', 'fucknigga', 'botch', 'brokeass', 'gtfoh', 'cockeyed', 'dyke', 'bitcccccch', 'stankin', 'muthafucka', 'gtf', 'bitchk', 'nigger', 'cornball', 'witta', 'forf', 'niggar', 'fym', 'bitchassness', 'prude', 'baldheaded', 'puzzy', 'worsum', 'darkie', 'innat', 'mothafucker', 'nigggga', 'stanking', 'getcho', 'beaner', 'fucktard', 'mandingo', 'baldhead', 'snitch', 'mfka']\n",
      "Positive values: \n",
      " ['latest', 'celebrate', 'performance', 'excited', 'enjoying', 'congratulations', 'day', 'inspired', 'sharing', 'appreciate', 'brazil', 'quality', 'nice', 'germany', 'exciting', 'good', 'present', 'inspiration', 'truly', 'blog', 'perfect', 'support', 'evening', 'wish', 'shared', 'favourite', 'thanks', 'hope', 'inspiring', 'lots', 'excellent', 'beautiful', 'wishes', 'loved', 'love', 'enjoyed', 'best', 'awesome', 'happy', 'special', 'superb', 'incredible', 'thank', 'share', 'lovely', 'enjoy', 'great', 'fantastic', 'amazing', 'wonderful']\n"
     ]
    }
   ],
   "source": [
    "print('Negative values: \\n', sorted_keys_densify[0:50])\n",
    "print('Positive values: \\n', sorted_keys_densify[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "alike-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys_socialsent_off = sorted(offensive_dict_socialsent, key=offensive_dict_socialsent.get)\n",
    "sorted_keys_densify_off = sorted(offensive_dict_densify, key=offensive_dict_densify.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "dominant-major",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values: \n",
      " ['nigga', 'motherfucker', 'asshole', 'skank', 'faggot', 'wetback', 'nigger', 'gringo', 'laat', 'chupacabra', 'redneck', 'dork', 'homie', 'puta', 'fucker', 'wiggy', 'bullshit', 'runt', 'goni', 'niggaz', 'jingo', 'mulatta', 'slut', 'shitty', 'denken', 'gooks', 'hoser', 'yowza', 'jigga', 'quisiera', 'crybaby', 'gurl', 'haha', 'onna', 'snitch', 'kaboom', 'hobo', 'goddamn', 'streetz', 'speedwagon', 'clich', 'no-brainer', 'drita', 'poppa', 'pasa', 'vivir', 'ohh', 'bitch', 'cunt', 'benzo']\n",
      "Positive values: \n",
      " ['skills', 'forever', 'performance', 'humble', 'neat', 'superb', 'quiet', 'exceptional', 'loves', 'impressive', 'perfectly', 'lucky', 'pleasure', 'passion', 'breezy', 'talent', 'cared', 'wonders', 'dreams', 'luck', 'beautifully', 'skill', 'grateful', 'elegant', 'proud', 'enjoy', 'enjoying', 'flawless', 'charming', 'lovers', 'dream', 'wonderful', 'brilliant', 'gorgeous', 'nice', 'touches', 'beauty', 'decent', 'beautiful', 'loving', 'fortunate', 'happy', 'good', 'delightful', 'loved', 'lovely', 'love', 'excellent', 'perfect', 'pleasant']\n"
     ]
    }
   ],
   "source": [
    "print('Negative values: \\n', sorted_keys_socialsent_off[0:50])\n",
    "print('Positive values: \\n', sorted_keys_socialsent_off[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "pressing-genius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values: \n",
      " ['yh', 'blac', 'grr', 'jacker', 'fpd', 'ey', 'hehe', 'cuh', 'rii', 'otf', 'kds', 'pasa', 'republicano', 'odia', 'drita', 'fml', 'bap', 'h-town', 'weet', 'samuela', 'ped', 'sav', 't-rex', 'sellin', 'nbp', 'chav', 't-bird', 'yardie', 'skank', 'pers', 'simp', 'joda', 'viri', 'shite', 'kik', 'mosa', 'wiggers', 'allisons', 'ily', 'chuckers', 'beaner', 'eww', 'lha', 'vap', 'gyp', 'mhm', 'pedo', 'uu', 'lls', 'boner']\n",
      "Positive values: \n",
      " ['spent', 'taking', 'always', 'though', 'none', 'seeing', 'home', 'room', 'seemed', 'giving', 'time', 'moment', 'beyond', 'staying', 'still', 'far', 'impressed', 'find', 'impressive', 'bringing', 'quite', 'excellent', 'perhaps', 'making', 'close', 'chance', 'brings', 'equally', 'way', 'much', 'rest', 'lot', 'place', 'enough', 'wonderful', 'interesting', 'looked', 'good', 'plenty', 'couple', 'yet', 'opportunity', 'moments', 'finding', 'looking', 'besides', 'comfortable', 'well', 'experience', 'shared']\n"
     ]
    }
   ],
   "source": [
    "print('Negative values: \\n', sorted_keys_densify_off[0:50])\n",
    "print('Positive values: \\n', sorted_keys_densify_off[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "assured-mystery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nigga',\n",
       " 'motherfucker',\n",
       " 'asshole',\n",
       " 'skank',\n",
       " 'faggot',\n",
       " 'wetback',\n",
       " 'nigger',\n",
       " 'gringo',\n",
       " 'laat',\n",
       " 'chupacabra',\n",
       " 'redneck',\n",
       " 'dork',\n",
       " 'homie',\n",
       " 'puta',\n",
       " 'fucker',\n",
       " 'wiggy',\n",
       " 'bullshit',\n",
       " 'runt',\n",
       " 'goni',\n",
       " 'niggaz',\n",
       " 'jingo',\n",
       " 'mulatta',\n",
       " 'slut',\n",
       " 'shitty',\n",
       " 'denken',\n",
       " 'gooks',\n",
       " 'hoser',\n",
       " 'yowza',\n",
       " 'jigga',\n",
       " 'quisiera',\n",
       " 'crybaby',\n",
       " 'gurl',\n",
       " 'haha',\n",
       " 'onna',\n",
       " 'snitch',\n",
       " 'kaboom',\n",
       " 'hobo',\n",
       " 'goddamn',\n",
       " 'streetz',\n",
       " 'speedwagon',\n",
       " 'clich',\n",
       " 'no-brainer',\n",
       " 'drita',\n",
       " 'poppa',\n",
       " 'pasa',\n",
       " 'vivir',\n",
       " 'ohh',\n",
       " 'bitch',\n",
       " 'cunt',\n",
       " 'benzo']"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_keys_socialsent_off[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-thanksgiving",
   "metadata": {},
   "source": [
    "Function to normalize the dictionary between -1 and 1 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "romantic-course",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nigger'"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hate_dict_socialsent.keys())[np.argmin(list(hate_dict_socialsent.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "artistic-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_dict(dictionary):\n",
    "\n",
    "#     value_list = list(dictionary.values())\n",
    "#     key_list = list(dictionary.keys())\n",
    "#     max_list = max(value_list)\n",
    "#     min_list = min(value_list)\n",
    "#     a = {key_list[i]: 2 * (value_list[i] - min_list)/(max_list - min_list) -1  for i in range(len(key_list))}\n",
    "#     return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "multiple-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hate\n",
    "# hate_dict_socialsent = normalize_dict(hate_dict_socialsent)\n",
    "# hate_dict_densify = normalize_dict(hate_dict_densify)\n",
    "\n",
    "# # offensive\n",
    "# offensive_dict_socialsent = normalize_dict(offensive_dict_socialsent)\n",
    "# offensive_dict_densify = normalize_dict(offensive_dict_densify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-georgia",
   "metadata": {},
   "source": [
    "Read each test file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "upset-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hate = pd.read_csv('hate_twits_test.csv')\n",
    "test_off = pd.read_csv('offensive_twits_test.csv')\n",
    "# test_hate = hate_twits.drop(columns='Unnamed: 0')\n",
    "# test_off = offensive_twits.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "raised-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize arrays for evaluation\n",
    "# ground truth\n",
    "hate_label = np.zeros(len(test_hate))\n",
    "offensive_label = np.zeros(len(test_off))\n",
    "# evaluated value\n",
    "hate_eval = np.zeros(len(test_hate))\n",
    "hate_offensive = np.zeros(len(test_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "obvious-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_dict_densify.get('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-reading",
   "metadata": {},
   "source": [
    "###### Pearson coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "designing-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, args):\n",
    "    evaluation_soc = 0  # socialsent\n",
    "    evaluation_dens = 0   # densify\n",
    "    counter = 0\n",
    "    if args == 'hate':\n",
    "        dict_soc = hate_dict_socialsent\n",
    "        dict_dens = hate_dict_densify\n",
    "    elif args == 'offensive':\n",
    "        dict_soc = offensive_dict_socialsent\n",
    "        dict_dens = offensive_dict_densify\n",
    "    dict_vector_soc = {}\n",
    "    dict_vector_dens = {}\n",
    "    for word in data['tweet'].split():\n",
    "        if dict_soc.get(word) is not None:\n",
    "            evaluation_soc += dict_soc.get(word)\n",
    "            evaluation_dens += dict_dens.get(word)\n",
    "            counter += 1\n",
    "            dict_vector_soc[word] = dict_soc.get(word)\n",
    "            dict_vector_dens[word] = dict_dens.get(word)\n",
    "    if counter == 0:\n",
    "        counter = 1   # avoid division by 0\n",
    "    data['vector_soc'] = dict_vector_soc\n",
    "    data['vector_dens'] = dict_vector_dens\n",
    "    data['evaluation_soc'] = 1 if evaluation_soc/counter >= 0.5 else -1\n",
    "    data['evaluation_dens'] = 1 if evaluation_dens/counter > 0 else -1\n",
    "    data['label_eval'] = 1 if data['label'] == 2 else -1   # 1 if type is neither, -1 if is hate/offensive\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "liked-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_twits_evaluated = test_hate.apply(evaluate, args=('hate',), axis=1)\n",
    "offensive_twits_evaluated = test_off.apply(evaluate, args=('offensive',), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "young-rescue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.22574736673722118, 2.138790155084657e-14)\n",
      "(0.23822591420434278, 6.656830529007784e-16)\n",
      "(0.15456233605962005, 2.2900494174449788e-26)\n",
      "(0.12622894280169195, 4.74134564950935e-18)\n",
      "report hate socialsent:                precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.94      0.31      0.46       869\n",
      "           1       0.28      0.93      0.43       250\n",
      "\n",
      "    accuracy                           0.45      1119\n",
      "   macro avg       0.61      0.62      0.45      1119\n",
      "weighted avg       0.79      0.45      0.46      1119\n",
      "\n",
      "report hate densify:                precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.54      0.34       134\n",
      "           1       0.93      0.78      0.85       985\n",
      "\n",
      "    accuracy                           0.75      1119\n",
      "   macro avg       0.59      0.66      0.60      1119\n",
      "weighted avg       0.84      0.75      0.79      1119\n",
      "\n",
      "report offensive socialsent:                precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.22      0.94      0.35       881\n",
      "           1       0.94      0.21      0.34      3790\n",
      "\n",
      "    accuracy                           0.35      4671\n",
      "   macro avg       0.58      0.58      0.35      4671\n",
      "weighted avg       0.80      0.35      0.34      4671\n",
      "\n",
      "report offensive densify:                precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.20      0.93      0.32       810\n",
      "           1       0.93      0.20      0.33      3861\n",
      "\n",
      "    accuracy                           0.33      4671\n",
      "   macro avg       0.56      0.56      0.33      4671\n",
      "weighted avg       0.80      0.33      0.33      4671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hate_eval_socialsent = stats.pearsonr(hate_twits_evaluated['evaluation_soc'], hate_twits_evaluated['label_eval'])\n",
    "hate_eval_densify = stats.pearsonr(hate_twits_evaluated['evaluation_dens'], hate_twits_evaluated['label_eval'])\n",
    "offensive_eval_socialsent = stats.pearsonr(offensive_twits_evaluated['evaluation_soc'], offensive_twits_evaluated['label_eval'])\n",
    "offensive_eval_densify = stats.pearsonr(offensive_twits_evaluated['evaluation_dens'], offensive_twits_evaluated['label_eval'])\n",
    "print(hate_eval_socialsent)\n",
    "print(hate_eval_densify)\n",
    "print(offensive_eval_socialsent)\n",
    "print(offensive_eval_densify)\n",
    "print('report hate socialsent: ', classification_report(hate_twits_evaluated['evaluation_soc'], hate_twits_evaluated['label_eval']))\n",
    "print('report hate densify: ', classification_report(hate_twits_evaluated['evaluation_dens'], hate_twits_evaluated['label_eval']))\n",
    "print('report offensive socialsent: ', classification_report(offensive_twits_evaluated['evaluation_soc'], offensive_twits_evaluated['label_eval']))\n",
    "print('report offensive densify: ', classification_report(offensive_twits_evaluated['evaluation_dens'], offensive_twits_evaluated['label_eval'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "statistical-vector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'made': 0.4949378937768095, 'greedy': 0.10941989327217277, 'faggot': 0.021329974704203693, 'atleast': 0.3638986162080378, 'broner': 0.48111176127377486}\n"
     ]
    }
   ],
   "source": [
    "print(hate_twits_evaluated[0:1]['vector_soc'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-portsmouth",
   "metadata": {},
   "source": [
    "###### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-tension",
   "metadata": {},
   "source": [
    "First, create a DictVectorizer object and create a vector with all the features( each word is a feature).\n",
    "Each twit is transformed into a dictionary with word:value, and then with all the dictionaries of all the train and test twits is created the vector mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "third-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join train and test into a dataset to make the dictionary for every instance\n",
    "hate_twits_all = train_hate.append(test_hate)\n",
    "off_twits_all = train_off.append(test_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "ranking-drinking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4474\n",
      "18682\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; yea. &lt;repeat&gt;what did it do for maidana? it made him a greedy faggot loser. atleast broner is black...</td>\n",
       "      <td>0</td>\n",
       "      <td>2636.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmao &lt;elong&gt; @ supreme being trash. guess bape is trash to huh?</td>\n",
       "      <td>2</td>\n",
       "      <td>12538.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt &lt;allcaps&gt; &lt;user&gt;: derek jeter leaves baseball as one of the greatest yankees ever. (via &lt;user&gt;) &lt;url&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>17396.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we over here struggling with college and work and ice jj fish making money for being trash lol. what has t...</td>\n",
       "      <td>2</td>\n",
       "      <td>22533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"the devil grows inside the hearts of the selfish and wicked\\nwhite, brown, yellow and black color is not ...</td>\n",
       "      <td>2</td>\n",
       "      <td>561.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>you said you be smoking strong but your wrong you been smoking trash it's not in my lungs if it ain't that...</td>\n",
       "      <td>2</td>\n",
       "      <td>23404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>rt &lt;allcaps&gt; &lt;user&gt;: every seth mcfarlene cartoon is trash yall love low iq comedy</td>\n",
       "      <td>2</td>\n",
       "      <td>16711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>&lt;user&gt; wha? me? i'm just waiting for showtime to bring back queer as folk.</td>\n",
       "      <td>2</td>\n",
       "      <td>7191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>yep, sources confirmed it. that iggy concert last night was trash</td>\n",
       "      <td>2</td>\n",
       "      <td>23154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>rt &lt;allcaps&gt; &lt;user&gt;: in anglo-saxon calendars, &lt;number&gt; november is considered the first day of winter - '...</td>\n",
       "      <td>2</td>\n",
       "      <td>14711.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1119 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              tweet  \\\n",
       "0     <user> yea. <repeat>what did it do for maidana? it made him a greedy faggot loser. atleast broner is black...   \n",
       "1                                                   lmao <elong> @ supreme being trash. guess bape is trash to huh?   \n",
       "2          rt <allcaps> <user>: derek jeter leaves baseball as one of the greatest yankees ever. (via <user>) <url>   \n",
       "3     we over here struggling with college and work and ice jj fish making money for being trash lol. what has t...   \n",
       "4     \"the devil grows inside the hearts of the selfish and wicked\\nwhite, brown, yellow and black color is not ...   \n",
       "...                                                                                                             ...   \n",
       "1114  you said you be smoking strong but your wrong you been smoking trash it's not in my lungs if it ain't that...   \n",
       "1115                             rt <allcaps> <user>: every seth mcfarlene cartoon is trash yall love low iq comedy   \n",
       "1116                                     <user> wha? me? i'm just waiting for showtime to bring back queer as folk.   \n",
       "1117                                              yep, sources confirmed it. that iggy concert last night was trash   \n",
       "1118  rt <allcaps> <user>: in anglo-saxon calendars, <number> november is considered the first day of winter - '...   \n",
       "\n",
       "      label  Unnamed: 0  \n",
       "0         0      2636.0  \n",
       "1         2     12538.0  \n",
       "2         2     17396.0  \n",
       "3         2     22533.0  \n",
       "4         2       561.0  \n",
       "...     ...         ...  \n",
       "1114      2     23404.0  \n",
       "1115      2     16711.0  \n",
       "1116      2      7191.0  \n",
       "1117      2     23154.0  \n",
       "1118      2     14711.0  \n",
       "\n",
       "[1119 rows x 3 columns]"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_hate))\n",
    "print(len(train_off))\n",
    "hate_twits_all[len(train_hate):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "integrated-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_value(data, args):\n",
    "    counter = 0\n",
    "    if args == 'hate':\n",
    "        dict_soc = hate_dict_socialsent\n",
    "        dict_dens = hate_dict_densify\n",
    "    elif args == 'offensive':\n",
    "        dict_soc = offensive_dict_socialsent\n",
    "        dict_dens = offensive_dict_densify\n",
    "    dict_vector_soc = {}   # socialsent\n",
    "    dict_vector_dens = {}  # densify\n",
    "    for word in data['tweet'].split():\n",
    "        if dict_soc.get(word) is not None:\n",
    "            dict_vector_soc[word] = dict_soc.get(word)\n",
    "            dict_vector_dens[word] = dict_dens.get(word)\n",
    "    data['vector_soc'] = dict_vector_soc\n",
    "    data['vector_dens'] = dict_vector_dens\n",
    "    data['label_eval'] = 1 if data['label'] == 2 else 0   # 1 if type is neither, -1 if is hate/offensive\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "settled-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_twits_all = hate_twits_all.apply(create_dict_value, args=('hate',), axis=1)\n",
    "off_twits_all = off_twits_all.apply(create_dict_value, args=('offensive',), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "featured-belgium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20047    {'rt': 0.5063806141046642, 'back': 0.5263003434024257, 'cause': 0.2677402089969097, 'see': 0.5334494047145...\n",
       "7465     {'random': 0.40529974965132687, 'scroll': 0.49962803387642474, 'charlie': 0.4868830320709538, 'horses': 0....\n",
       "21686    {'side': 0.4315926665743458, 'trip': 0.7697714431763033, 'dc': 0.49483151480534737, 'charlie': 0.486883032...\n",
       "9196     {'fresh': 0.6160448856578564, 'gym': 0.5376617430152593, 'yet': 0.5258872116549106, 'bout': 0.121558781753...\n",
       "7841     {'pussy': 0.09616483947559788, 'lounge': 0.617986187862427, 'killin': 0.15160235388139465, 'girlz': 0.1808...\n",
       "                                                             ...                                                      \n",
       "1114     {'said': 0.47912170683458416, 'smoking': 0.30964937457548797, 'strong': 0.6116551535552796, 'wrong': 0.490...\n",
       "1115     {'rt': 0.5063806141046642, 'every': 0.549755445910003, 'seth': 0.4904054306915105, 'cartoon': 0.5449714493...\n",
       "1116     {'waiting': 0.5523911135733949, 'showtime': 0.5685475254006874, 'bring': 0.5105451312343443, 'back': 0.526...\n",
       "1117     {'sources': 0.5765458351935271, 'confirmed': 0.5452714531734874, 'it.': 0.18907864216120737, 'iggy': 0.391...\n",
       "1118     {'rt': 0.5063806141046642, 'anglo-saxon': 0.22463718891857581, 'november': 0.5122045584576929, 'considered...\n",
       "Name: vector_soc, Length: 5593, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (5593,)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_colwidth', 110)\n",
    "display(hate_twits_all['vector_soc'])\n",
    "print('shape: ', hate_twits_all['vector_soc'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-lecture",
   "metadata": {},
   "source": [
    "Create the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "suburban-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = DictVectorizer(sparse=True)\n",
    "v_hate_soc = v.fit_transform(hate_twits_all['vector_soc'])\n",
    "v_hate_dens = v.fit_transform(hate_twits_all['vector_dens'])\n",
    "\n",
    "v_off_soc = v.fit_transform(off_twits_all['vector_soc'])\n",
    "v_off_dens = v.fit_transform(off_twits_all['vector_dens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "neutral-christianity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5593, 7522)\n",
      "(5593, 7522)\n",
      "(23353, 12111)\n",
      "(23353, 12111)\n"
     ]
    }
   ],
   "source": [
    "print(v_hate_soc.shape)\n",
    "print(v_hate_dens.shape)\n",
    "print(v_off_soc.shape)\n",
    "print(v_off_dens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "round-elements",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 428)\t0.5263003434024257\n",
      "  (0, 793)\t0.3164999213872843\n",
      "  (0, 1064)\t0.2677402089969097\n",
      "  (0, 3885)\t0.47165527548585323\n",
      "  (0, 5578)\t0.5063806141046642\n",
      "  (0, 5758)\t0.5334494047145935\n",
      "  (0, 6823)\t0.21597066359161696\n",
      "  (1, 1124)\t0.4868830320709538\n",
      "  (1, 1689)\t0.8236881043634942\n",
      "  (1, 1987)\t0.38211414617804745\n",
      "  (1, 2155)\t0.5014826966725354\n",
      "  (1, 3134)\t0.3535550463720502\n",
      "  (1, 4071)\t0.4747581413735531\n",
      "  (1, 5259)\t0.40529974965132687\n",
      "  (1, 5733)\t0.49962803387642474\n",
      "  (1, 5815)\t0.21251174491134672\n",
      "  (1, 5938)\t0.5873525549082252\n",
      "  (1, 7205)\t0.3818910016657658\n",
      "  (2, 1124)\t0.4868830320709538\n",
      "  (2, 1521)\t0.30862250023755056\n",
      "  (2, 1694)\t0.49483151480534737\n",
      "  (2, 4705)\t0.3183618464339526\n",
      "  (2, 5922)\t0.4315926665743458\n",
      "  (2, 6860)\t0.7697714431763033\n",
      "  (3, 780)\t0.12155878175398618\n",
      "  :\t:\n",
      "  (5589, 3907)\t0.504886118759416\n",
      "  (5589, 5578)\t0.5063806141046642\n",
      "  (5589, 5805)\t0.4904054306915105\n",
      "  (5589, 6823)\t0.21597066359161696\n",
      "  (5589, 7447)\t0.09671093351529594\n",
      "  (5590, 428)\t0.5263003434024257\n",
      "  (5590, 834)\t0.5105451312343443\n",
      "  (5590, 5203)\t0.039185478172258426\n",
      "  (5590, 5911)\t0.5685475254006874\n",
      "  (5590, 7161)\t0.5523911135733949\n",
      "  (5591, 1395)\t0.6454754263441689\n",
      "  (5591, 1407)\t0.5452714531734874\n",
      "  (5591, 3229)\t0.39136280930527584\n",
      "  (5591, 3381)\t0.18907864216120737\n",
      "  (5591, 3661)\t0.6784415536126704\n",
      "  (5591, 4451)\t0.777191238529121\n",
      "  (5591, 6134)\t0.5765458351935271\n",
      "  (5591, 6823)\t0.21597066359161696\n",
      "  (5592, 225)\t0.22463718891857581\n",
      "  (5592, 1427)\t0.49285551697662755\n",
      "  (5592, 1689)\t0.8236881043634942\n",
      "  (5592, 2412)\t0.6424349303598343\n",
      "  (5592, 4503)\t0.5122045584576929\n",
      "  (5592, 5578)\t0.5063806141046642\n",
      "  (5592, 7341)\t0.6160515508059047\n",
      "shape;  (5593, 7522)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(edgeitems=8)\n",
    "print(v_hate_soc)\n",
    "print('shape; ', v_hate_soc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-cabinet",
   "metadata": {},
   "source": [
    "Fit a logisitic classifier with the training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "willing-crest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (4474, 7522)\n",
      "Test:  (1119, 7522)\n",
      "Total:  (5593, 7522)\n"
     ]
    }
   ],
   "source": [
    "print('Train: ', v_hate_soc[:len(train_hate)].shape)\n",
    "print('Test: ', v_hate_soc[len(train_hate):].shape)\n",
    "print('Total: ', v_hate_soc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "running-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_hate_soc = LogisticRegression( random_state=0).fit(v_hate_soc[:len(train_hate)], hate_twits_all['label_eval'][:len(train_hate)])\n",
    "clf_hate_dens = LogisticRegression( random_state=0).fit(v_hate_dens[:len(train_hate)], hate_twits_all['label_eval'][:len(train_hate)])\n",
    "clf_off_soc = LogisticRegression(max_iter = 5000, random_state=0).fit(v_off_soc[:len(train_off)], off_twits_all['label_eval'][:len(train_off)])\n",
    "clf_off_dens = LogisticRegression(max_iter = 5000, random_state=0).fit(v_off_dens[:len(train_off)], off_twits_all['label_eval'][:len(train_off)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-affairs",
   "metadata": {},
   "source": [
    "Predict class of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "educational-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_hate_soc = clf_hate_soc.predict(v_hate_soc[len(train_hate):])\n",
    "y_score_hate_dens = clf_hate_dens.predict(v_hate_dens[len(train_hate):])\n",
    "y_score_off_soc = clf_off_soc.predict(v_off_soc[len(train_off):])\n",
    "y_score_off_dens = clf_off_dens.predict(v_off_dens[len(train_off):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "catholic-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_hate_soc = average_precision_score(hate_twits_all['label_eval'][len(train_hate):], y_score_hate_soc)\n",
    "precision_hate_dens = average_precision_score(hate_twits_all['label_eval'][len(train_hate):], y_score_hate_dens)\n",
    "precision_off_soc = average_precision_score(off_twits_all['label_eval'][len(train_off):], y_score_off_soc)\n",
    "precision_off_dens = average_precision_score(off_twits_all['label_eval'][len(train_off):], y_score_off_dens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "unique-extra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision hate socialsent:  0.7978482906232198\n",
      "Precision hate densify:  0.8983355298745312\n",
      "Precision offensive socialsent:  0.5741220443179291\n",
      "Precision offensive densify:  0.5976600189309137\n"
     ]
    }
   ],
   "source": [
    "print('Precision hate socialsent: ', precision_hate_soc)\n",
    "print('Precision hate densify: ',precision_hate_dens)\n",
    "print('Precision offensive socialsent: ',precision_off_soc)\n",
    "print('Precision offensive densify: ',precision_off_dens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "moving-madonna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.40956153269530765, 1.690701798089343e-46)\n",
      "(0.7283164897672423, 1.4193624106992377e-185)\n",
      "(0.6590951695142439, 0.0)\n",
      "(0.6835435775264599, 0.0)\n",
      "Report hate soc:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.28      0.42       286\n",
      "           1       0.80      0.98      0.88       833\n",
      "\n",
      "    accuracy                           0.80      1119\n",
      "   macro avg       0.82      0.63      0.65      1119\n",
      "weighted avg       0.81      0.80      0.76      1119\n",
      "\n",
      "Report hate dens:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.70      0.78       286\n",
      "           1       0.90      0.97      0.94       833\n",
      "\n",
      "    accuracy                           0.90      1119\n",
      "   macro avg       0.90      0.83      0.86      1119\n",
      "weighted avg       0.90      0.90      0.90      1119\n",
      "\n",
      "Report off soc:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95      3838\n",
      "           1       0.85      0.59      0.70       833\n",
      "\n",
      "    accuracy                           0.91      4671\n",
      "   macro avg       0.88      0.78      0.82      4671\n",
      "weighted avg       0.90      0.91      0.90      4671\n",
      "\n",
      "Report off dens:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.95      3838\n",
      "           1       0.79      0.68      0.73       833\n",
      "\n",
      "    accuracy                           0.91      4671\n",
      "   macro avg       0.86      0.82      0.84      4671\n",
      "weighted avg       0.91      0.91      0.91      4671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(stats.pearsonr(hate_twits_all['label_eval'][len(train_hate):], y_score_hate_soc))\n",
    "print(stats.pearsonr(hate_twits_all['label_eval'][len(train_hate):], y_score_hate_dens))\n",
    "print(stats.pearsonr(off_twits_all['label_eval'][len(train_off):], y_score_off_soc))\n",
    "print(stats.pearsonr(off_twits_all['label_eval'][len(train_off):], y_score_off_dens))\n",
    "print('Report hate soc: ', classification_report(hate_twits_all['label_eval'][len(train_hate):], y_score_hate_soc))\n",
    "print('Report hate dens: ',classification_report(hate_twits_all['label_eval'][len(train_hate):], y_score_hate_dens))\n",
    "print('Report off soc: ',classification_report(off_twits_all['label_eval'][len(train_off):], y_score_off_soc))\n",
    "print('Report off dens: ',classification_report(off_twits_all['label_eval'][len(train_off):], y_score_off_dens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "eleven-condition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1119.000000\n",
       "mean        0.744415\n",
       "std         0.436385\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         1.000000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "Name: label_eval, dtype: float64"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_twits_all['label_eval'][len(train_hate):].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-oxygen",
   "metadata": {},
   "source": [
    "###### Classification on unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-blackberry",
   "metadata": {},
   "source": [
    "Create a matrix of the ocurrences each word occurs in a twit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "sustainable-breeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19102,)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "unigram_ocurrence_matrix = vectorizer.fit_transform(hate_twits_all['tweet'])\n",
    "unigram_ocurrence_matrix_off = vectorizer.fit_transform(off_twits_all['tweet'])\n",
    "#print(vectorizer.get_feature_names())\n",
    "print(unigram_ocurrence_matrix_off.toarray()[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-bangkok",
   "metadata": {},
   "source": [
    "Fit a logistic regresor with this features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "hearing-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_unigram = LogisticRegression(random_state=0).fit(unigram_ocurrence_matrix[:len(train_hate)], hate_twits_all['label_eval'][:len(train_hate)])\n",
    "clf_unigram_off = LogisticRegression(random_state=0, max_iter=5000).fit(unigram_ocurrence_matrix_off[:len(train_off)], off_twits_all['label_eval'][:len(train_off)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "played-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_unigram = clf_unigram.predict(unigram_ocurrence_matrix[len(train_hate):])\n",
    "y_score_unigram_off = clf_unigram_off.predict(unigram_ocurrence_matrix_off[len(train_off):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "union-reproduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8046536737951281, 3.8256357708205807e-255)\n",
      "(0.8591798580421246, 0.0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.79      0.85       286\n",
      "           1       0.93      0.97      0.95       833\n",
      "\n",
      "    accuracy                           0.93      1119\n",
      "   macro avg       0.92      0.88      0.90      1119\n",
      "weighted avg       0.93      0.93      0.93      1119\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      3838\n",
      "           1       0.86      0.91      0.88       833\n",
      "\n",
      "    accuracy                           0.96      4671\n",
      "   macro avg       0.92      0.94      0.93      4671\n",
      "weighted avg       0.96      0.96      0.96      4671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precision_unigram = average_precision_score(hate_twits_all['label_eval'][len(train_hate):], y_score_unigram)\n",
    "precision_unigram_off = average_precision_score(off_twits_all['label_eval'][len(train_off):], y_score_unigram_off)\n",
    "precision_unigram\n",
    "print(stats.pearsonr(hate_twits_all['label_eval'][len(train_hate):], y_score_unigram))\n",
    "print(stats.pearsonr(off_twits_all['label_eval'][len(train_off):], y_score_unigram_off))\n",
    "print(classification_report(hate_twits_all['label_eval'][len(train_hate):], y_score_unigram))\n",
    "print(classification_report(off_twits_all['label_eval'][len(train_off):], y_score_unigram_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "widespread-upgrade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x10787 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 18 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "angry-upgrade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20047       rt <allcaps> <user>: i'm only back cause i don't wanna see the boys lose to trash <hastag> vote<number>sos\n",
       "7465     a random am scroll lands me here. <repeat>rt <allcaps> <user> charlie horses durin sex just simply means y...\n",
       "21686    there should be a side trip into dc <allcaps> to charlie palmers during cpac <allcaps> <user> <user> <user...\n",
       "9196                                                             fresh out the gym but yet i'm bout to eat free birds.\n",
       "7841                                                     at pussy cat lounge killin it with my girlz <user> and <user>\n",
       "                                                             ...                                                      \n",
       "5068     <user> hey i'll leave abbey too! oh if i get a job on the slope i might move back! cause id lose out on lo...\n",
       "3330                                <user> <user> bc <user> is trash why would he lie about that? i told you he's real\n",
       "2500                                                              <user> answer my snapchat faggot \\n<hastag> butthurt\n",
       "2486     <user> welcome to the movement. anyone who gets even minor publicity is a queer, fed, or jew (well, accord...\n",
       "23606                                                                                  beanies cumming real soon <url>\n",
       "Name: tweet, Length: 4474, dtype: object"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_twits_all['tweet'][:len(train_hate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "smooth-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "6       0\n",
       "12      0\n",
       "14      0\n",
       "17      0\n",
       "       ..\n",
       "1093    0\n",
       "1097    0\n",
       "1100    0\n",
       "1112    0\n",
       "1113    0\n",
       "Name: label_eval, Length: 286, dtype: int64"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_twits_all['label_eval'][len(train_hate):][hate_twits_all['label_eval'][len(train_hate):] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "settled-puppy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49625832763748035"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_dict_socialsent.get('think')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
