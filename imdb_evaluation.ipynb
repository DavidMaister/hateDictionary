{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "ideal-retrieval",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-542-970d4fd9f46c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# open the file  with the review\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_review\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# file only contain a single line with the review\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStopWordsRemover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# write a line in the final file with the review\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/gsitk/preprocess/stopwords.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mtransformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransformed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/gsitk/preprocess/stopwords.py\u001b[0m in \u001b[0;36mremove_stopwords\u001b[0;34m(self, tokens, stop_words)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/gsitk/preprocess/stopwords.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "from gsitk.preprocess.stopwords import StopWordsRemover\n",
    "# file to store the reviews line by line\n",
    "# opening and writing files with this syntax to encode with utf-8 and not getting ascii errors\n",
    "with io.open('imdb_reviews.txt', 'w', encoding='utf8') as f:\n",
    "    counter = 0\n",
    "    for review in os.listdir('aclImdb/train/pos'):\n",
    "        with io.open('aclImdb/train/pos/' + review, 'r', encoding='utf8') as f_review:\n",
    "        # open the file  with the review\n",
    "            text = f_review.read()  # file only contain a single line with the review\n",
    "            text = StopWordsRemover().fit_transform([text.lower()])[0]\n",
    "            \n",
    "            f.write(text + '\\n')    # write a line in the final file with the review\n",
    "        #f_review.close()\n",
    "#         if counter > 4000:\n",
    "#             break\n",
    "#         counter += 1\n",
    "\n",
    "    counter = 0\n",
    "    for review in os.listdir('aclImdb/train/neg'):  # same for for positive than wth negative\n",
    "        with io.open('aclImdb/train/neg/' + review, 'r', encoding='utf8') as f_review:\n",
    "            # open the file  with the review\n",
    "            text = f_review.read()  # file only contain a single line with the review\n",
    "            text = StopWordsRemover().fit_transform([text.lower()])[0]\n",
    "            f.write(text + '\\n')  # write a line in the final file with the review\n",
    "        #f_review.close()\n",
    "#         if counter > 4000:\n",
    "#             break\n",
    "#         counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open('imdb_reviews.txt', 'r', encoding='utf8') as corpus:\n",
    "    lines = corpus.readlines()\n",
    "    print(lines[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "with io.open('aclImdb/train/pos/' + os.listdir('aclImdb/train/pos')[0], 'r', encoding='utf8') as f_review:\n",
    "        # open the file  with the review\n",
    "        text = f_review.read()  # file only contain a single line with the review\n",
    "        print(text.__class__)\n",
    "        print(StopWordsRemover().fit_transform([text])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-tongue",
   "metadata": {},
   "source": [
    "##### Execute 'hatedictionary.py imdb_reviews.txt imdb' before continuing with the next part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-printing",
   "metadata": {},
   "source": [
    "# IMDB evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from gsitk.preprocess.stopwords import StopWordsRemover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-information",
   "metadata": {},
   "source": [
    "We are going to create the test dataset. We iterate randdomly through the dataset and take equal number of positive reviews than negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = 1000\n",
    "\n",
    "file_list_pos = random.sample(os.listdir('aclImdb/test/pos'), n_files)  # select x random files\n",
    "file_list_neg = random.sample(os.listdir('aclImdb/test/neg'), n_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-atlas",
   "metadata": {},
   "source": [
    "Create the groundtruth lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_gt = np.append(np.ones(n_files),np.ones(n_files)*-1)  # ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-personal",
   "metadata": {},
   "source": [
    "get the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionaries/imdb2_densify.json') as json_file:\n",
    "    dict_densify = json.load(json_file)\n",
    "with open('dictionaries/imdb2_socialsent.json') as json_file:\n",
    "    dict_socialsent = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_socialsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys_socialsent = sorted(dict_socialsent, key=dict_socialsent.get)\n",
    "sorted_keys_densify= sorted(dict_densify, key=dict_densify.get)\n",
    "\n",
    "\n",
    "wanted_keys = sorted_keys_socialsent[:3000] + sorted_keys_socialsent[-3000:]\n",
    "dict_socialsent = dict((k, dict_socialsent[k]) for k in wanted_keys)\n",
    "wanted_keys_dens = sorted_keys_densify[:3000] + sorted_keys_socialsent[-3000:]\n",
    "dict_densify = dict((k, dict_densify[k]) for k in wanted_keys_dens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_keys = sorted_keys_socialsent[:3000] + sorted_keys_socialsent[-3000:]\n",
    "dict_socialsent = dict((k, dict_socialsent[k]) for k in wanted_keys)\n",
    "wanted_keys_dens = sorted_keys_densify[:3000] + sorted_keys_socialsent[-3000:]\n",
    "dict_densify = dict((k, dict_densify[k]) for k in wanted_keys_dens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-treaty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys_socialsent[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys_socialsent[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(list(dict_densify.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-economics",
   "metadata": {},
   "source": [
    "Function to normalize the dictionary between -1 and 1 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_dict(dictionary):\n",
    "\n",
    "#     value_list = list(dictionary.values())\n",
    "#     key_list = list(dictionary.keys())\n",
    "#     max_list = max(value_list)\n",
    "#     min_list = min(value_list)\n",
    "#     a = {key_list[i]: 2 * (value_list[i] - min_list)/(max_list - min_list) -1  for i in range(len(key_list))}\n",
    "#     return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_socialsent = normalize_dict(dict_socialsent)\n",
    "#dict_densify = normalize_dict(dict_densify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-finnish",
   "metadata": {},
   "source": [
    "Read each test positive file and calculate the mean value for both densify and socialsent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-calculator",
   "metadata": {},
   "source": [
    "list_pos_socialsent = np.zeros(n_files)\n",
    "list_pos_densify = np.zeros(n_files)\n",
    "stop_words = set(stopwords.words())   # convert into a set for speed purposes\n",
    "i = 0\n",
    "for file in file_list_pos:\n",
    "    with io.open('aclImdb/test/pos/' + file, 'r', encoding='utf8') as review_file:\n",
    "        review = review_file.read() # read the file\n",
    "    words = review.split()\n",
    "    sum_file_socialsent = 0\n",
    "    sum_file_densify = 0\n",
    "    counter_socialsent = 0\n",
    "    counter_densify = 0\n",
    "    for word in words:\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            if dict_socialsent.get(word) is not None:\n",
    "                sum_file_socialsent += dict_socialsent.get(word)\n",
    "                counter_socialsent += 1\n",
    "            if dict_densify.get(word) is not None:    \n",
    "                sum_file_densify += dict_densify.get(word)\n",
    "                counter_densify += 1\n",
    "    list_pos_socialsent[i] = 1 if sum_file_socialsent / counter_socialsent >= 0.5 else -1\n",
    "    list_pos_densify[i] = 1 if sum_file_densify / counter_densify > 0 else -1\n",
    "    i += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-richards",
   "metadata": {},
   "source": [
    "same for negative files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-cherry",
   "metadata": {},
   "source": [
    "list_neg_socialsent = np.zeros(n_files)\n",
    "list_neg_densify = np.zeros(n_files)\n",
    "stop_words = set(stopwords.words())   # convert into a set for speed purposes\n",
    "i = 0\n",
    "for file in file_list_neg:\n",
    "    with io.open('aclImdb/test/neg/' + file, 'r', encoding='utf8') as review_file:\n",
    "        review = review_file.read() # read the file\n",
    "    words = review.split()\n",
    "    sum_file_socialsent = 0\n",
    "    sum_file_densify = 0\n",
    "    counter_socialsent = 0\n",
    "    counter_densify = 0\n",
    "    for word in words:\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            if dict_socialsent.get(word) is not None:\n",
    "                sum_file_socialsent += dict_socialsent.get(word)\n",
    "                counter_socialsent += 1\n",
    "            if dict_densify.get(word) is not None:    \n",
    "                sum_file_densify += dict_densify.get(word)\n",
    "                counter_densify += 1\n",
    "    list_neg_socialsent[i] = sum_file_socialsent / counter_socialsent\n",
    "    list_neg_densify[i] = sum_file_densify / counter_densify\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-exclusive",
   "metadata": {},
   "source": [
    "Join the positive and negative lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_socialsent = np.append(list_pos_socialsent, list_neg_socialsent)\n",
    "list_densify = np.append(list_pos_densify, list_neg_densify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-complement",
   "metadata": {},
   "source": [
    "Evaluate with pearson coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_socialsent = stats.pearsonr(list_gt, list_socialsent)\n",
    "evaluation_densify = stats.pearsonr(list_gt, list_densify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Socialsent: ', evaluation_socialsent)\n",
    "print('Densify: ', evaluation_densify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(list_socialsent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-reason",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-dover",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "separate-plane",
   "metadata": {},
   "source": [
    "# this way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-night",
   "metadata": {},
   "source": [
    "##### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "transparent-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_train_pos = os.listdir('aclImdb/train/pos')\n",
    "files_train_neg = os.listdir('aclImdb/train/neg')\n",
    "files_test_pos = os.listdir('aclImdb/test/pos')\n",
    "files_test_neg = os.listdir('aclImdb/test/neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-textbook",
   "metadata": {},
   "source": [
    "Read every file and save the information in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "personalized-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_imdb_dataframe(file_list, file_path, label, mode, imdb_list):   # path example: 'aclImdb/train/neg/'\n",
    "    for file in file_list:  \n",
    "        with io.open(file_path + file, 'r', encoding='utf8') as file_review:\n",
    "            review = file_review.read() # read the file\n",
    "            review = StopWordsRemover().fit_transform([review.lower()])\n",
    "        dict_aux = {}\n",
    "        dict_aux['review'] = review[0]\n",
    "        dict_aux['label'] = label\n",
    "        dict_aux['set'] = mode    # train or test\n",
    "        imdb_list.append(dict_aux)\n",
    "    return imdb_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-testimony",
   "metadata": {},
   "source": [
    "The label 1 is a positive review and the label '0' a negative one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "rough-hunger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guy desperate action attempts hit gorgeous gir...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ahista ahista one little small brilliant. star...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wracked guilt lot things felt apart ledge, ace...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>think hollow point funny film good moments nev...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dana andrews stands \"where sidewalk ends\" 1950...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>spoiler pants thinking movie made angry crappe...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>meltdown opens scene scientists preparing cond...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>spoilers&lt;br /&gt;&lt;br /&gt;this movie action packed; ...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>people say children annoying u think ya little...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>boooooooorrrrrinngggggggg stooooooopiddddd. ke...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label    set\n",
       "0      guy desperate action attempts hit gorgeous gir...      1  train\n",
       "1      ahista ahista one little small brilliant. star...      1  train\n",
       "2      wracked guilt lot things felt apart ledge, ace...      1  train\n",
       "3      think hollow point funny film good moments nev...      1  train\n",
       "4      dana andrews stands \"where sidewalk ends\" 1950...      1  train\n",
       "...                                                  ...    ...    ...\n",
       "49995  spoiler pants thinking movie made angry crappe...      0   test\n",
       "49996  meltdown opens scene scientists preparing cond...      0   test\n",
       "49997  spoilers<br /><br />this movie action packed; ...      0   test\n",
       "49998  people say children annoying u think ya little...      0   test\n",
       "49999  boooooooorrrrrinngggggggg stooooooopiddddd. ke...      0   test\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_list = []   # store all the information, then it is converted to a dataframe\n",
    "imdb_list = build_imdb_dataframe(files_train_pos, 'aclImdb/train/pos/', 1, 'train', imdb_list)\n",
    "imdb_list = build_imdb_dataframe(files_train_neg, 'aclImdb/train/neg/', 0, 'train', imdb_list)\n",
    "imdb_list = build_imdb_dataframe(files_test_pos, 'aclImdb/test/pos/', 1, 'test', imdb_list)\n",
    "imdb_list = build_imdb_dataframe(files_test_neg, 'aclImdb/test/neg/', 0, 'test', imdb_list)\n",
    "imdb_reviews = pd.DataFrame(imdb_list)\n",
    "imdb_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "id": "small-audience",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321      i'm male, given women's movies, really well do...\n",
       "790      read comments tomreynolds2004 feel jump here. ...\n",
       "5541     although like stanley & iris tremendously film...\n",
       "5912     working-class romantic drama director martin r...\n",
       "6462     may memorable classic, touching romance import...\n",
       "6722     'stanley iris' show triumph human spirit. stan...\n",
       "8363     film special place heart, caught first time, t...\n",
       "9456     production quality, cast, premise, authentic n...\n",
       "9586     found so-so romance/drama nice ending generall...\n",
       "13912    given people involved, hard see movie messed d...\n",
       "15674    saw capsule comment said \"great acting.\" opini...\n",
       "17517    watched movie times never really thought funny...\n",
       "18995    robert deniro plays unbelievably intelligent i...\n",
       "20477    uncertain make misshapen 2007 dramedy. attempt...\n",
       "22116    disliked movie numerous reasons. within first ...\n",
       "23557    georgia rule got one - worst movie ever seen l...\n",
       "25435    rented \"the china syndrome\" recently mainly re...\n",
       "25623    tv newscaster kimberly wells (jane fonda) radi...\n",
       "25809    wish hollywood would make movies like china sy...\n",
       "27224    large corporations vs. conscientious good-ers....\n",
       "27447    china syndrome perfectly paced thriller slow b...\n",
       "27962    \"the china syndrome\" launched whole string fil...\n",
       "28073    think not! mean yeah compare film godfather, m...\n",
       "28523    reporter kimberly wells presents minor side ne...\n",
       "30602    james bridges' \"the china syndrome\" first rate...\n",
       "31245    normally comment movies imdb, case feel like s...\n",
       "31386    intelligent, nail-biting drama came nowhere 19...\n",
       "31478    harrowingly close, china syndrome received wor...\n",
       "32277    one best exciting conspiracy thrillers, (there...\n",
       "33209    great film late 1970's. says much corporate co...\n",
       "33401    again, i've read comments posted agree many in...\n",
       "34100    \"the china syndrome\" could released better tim...\n",
       "35647    years since i've seen movie, forget details. h...\n",
       "35876    highly politically charged drama that, biased,...\n",
       "36246    long mind paying little attention normally mig...\n",
       "36701    pretty good thriller nuclear power plant south...\n",
       "39830    can't liberals like alec baldwin get heads los...\n",
       "41701    affair bad tv movie 1970s starring then-husban...\n",
       "42840    oh lord, thinking one. frantically unfunny, wo...\n",
       "43616    reassuring see imdb reviewers good sense pan d...\n",
       "45140    terrible misfire. title idea jane fonda, georg...\n",
       "45239    waited long time finally see thought going fun...\n",
       "46976    feminist tract viewer believe that: i) wild an...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 960,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_reviews[imdb_reviews['review'].str.contains(\"jane fonda\")]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "id": "coordinated-richmond",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'm male, given women's movies, really well done special story. personal love jane fonda person one hell fine job, deniro usual superb self. everything well done: acting, directing, visuals, settings, photography, casting. enjoy story real people real love - winner.\""
      ]
     },
     "execution_count": 961,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_reviews['review'][321]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-dating",
   "metadata": {},
   "source": [
    "Create a text file with a limited number of samples, for train and for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "laden-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = imdb_reviews[imdb_reviews['set'] == 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "unsigned-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = imdb_reviews[imdb_reviews['set'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "material-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_reviews = train_set.append(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "comparable-venezuela",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-karaoke",
   "metadata": {},
   "source": [
    "Save the train to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "expired-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['review'].to_csv('imdb_reviews.txt', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-islam",
   "metadata": {},
   "source": [
    "###### Classification on unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "endangered-dance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 21045)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=15)\n",
    "unigram_ocurrence_matrix = vectorizer.fit_transform(imdb_reviews['review'])\n",
    "#print(vectorizer.get_feature_names())\n",
    "print(unigram_ocurrence_matrix.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "norman-proceeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 1 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 ... 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]]\n",
      "shape:  (50000, 21045)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=1000, edgeitems=10)\n",
    "print(unigram_ocurrence_matrix.toarray())\n",
    "print('shape: ', unigram_ocurrence_matrix.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "thermal-disclosure",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 21045)"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.sum(unigram_ocurrence_matrix, axis=0)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "danish-intranet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x21045 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2450886 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(a)\n",
    "unigram_ocurrence_matrix[imdb_reviews['set'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "difficult-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_unigram = LogisticRegression(random_state=0, max_iter=8000).fit(unigram_ocurrence_matrix[imdb_reviews['set'] == 'train'], \n",
    "                                                                    imdb_reviews[imdb_reviews['set'] == 'train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "alive-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_unigram = clf_unigram.predict(unigram_ocurrence_matrix[imdb_reviews['set'] == 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "id": "moderate-transportation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86     12693\n",
      "           1       0.85      0.87      0.86     12307\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n",
      "(0.7220860754797889, 0.0)\n"
     ]
    }
   ],
   "source": [
    "precision_unigram = average_precision_score(imdb_reviews[imdb_reviews['set'] == 'test']['label'], y_score_unigram)\n",
    "print(classification_report(y_score_unigram, imdb_reviews[imdb_reviews['set'] == 'test']['label'] ))\n",
    "print(stats.pearsonr(y_score_unigram, imdb_reviews[imdb_reviews['set'] == 'test']['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-contrast",
   "metadata": {},
   "source": [
    "###### Logistic regression using the dictionary values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-leader",
   "metadata": {},
   "source": [
    "Get the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "continental-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hate\n",
    "with open('dictionaries/imdb2_densify.json') as json_file:\n",
    "    dict_densify = json.load(json_file)\n",
    "with open('dictionaries/imdb2_socialsent.json') as json_file:\n",
    "    dict_socialsent = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "coordinated-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_keys_socialsent = sorted(dict_socialsent, key=dict_socialsent.get)\n",
    "# sorted_keys_densify= sorted(dict_densify, key=dict_densify.get)\n",
    "\n",
    "\n",
    "# wanted_keys = sorted_keys_socialsent[:3000] + sorted_keys_socialsent[-3000:]\n",
    "# dict_socialsent = dict((k, dict_socialsent[k]) for k in wanted_keys)\n",
    "# wanted_keys_dens = sorted_keys_densify[:3000] + sorted_keys_socialsent[-3000:]\n",
    "# dict_densify = dict((k, dict_densify[k]) for k in wanted_keys_dens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "fatal-keyboard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63514\n",
      "63514\n"
     ]
    }
   ],
   "source": [
    "print(len(list(dict_socialsent.values())))\n",
    "print(len(list(dict_densify.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "fluid-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_value(data):\n",
    "    counter = 0\n",
    "\n",
    "    dict_vector_soc = {}   # socialsent\n",
    "    dict_vector_dens = {}  # densify\n",
    "    for word in data['review'].split():\n",
    "        if dict_socialsent.get(word) is not None:\n",
    "            dict_vector_soc[word] = dict_socialsent.get(word)\n",
    "            dict_vector_dens[word] = dict_densify.get(word)\n",
    "    data['vector_soc'] = dict_vector_soc\n",
    "    data['vector_dens'] = dict_vector_dens\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "wound-schema",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "      <th>vector_soc</th>\n",
       "      <th>vector_dens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guy desperate action attempts hit gorgeous gir...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>{'guy': 0.9481098584732381, 'desperate': 0.813...</td>\n",
       "      <td>{'guy': 0.14247038960456848, 'desperate': 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ahista ahista one little small brilliant. star...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>{'little': 0.9747448137394689, 'small': 0.9277...</td>\n",
       "      <td>{'little': 0.34360048174858093, 'small': 0.604...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wracked guilt lot things felt apart ledge, ace...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>{'wracked': 0.5912880078969615, 'guilt': 0.635...</td>\n",
       "      <td>{'wracked': -0.3791770935058594, 'guilt': 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>think hollow point funny film good moments nev...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>{'think': 0.9601986716024294, 'hollow': 0.7854...</td>\n",
       "      <td>{'think': 0.16891370713710785, 'hollow': 0.324...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dana andrews stands \"where sidewalk ends\" 1950...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>{'dana': 0.5221360341507066, 'andrews': 0.5030...</td>\n",
       "      <td>{'dana': 0.6259295344352722, 'andrews': 0.6521...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>spoiler pants thinking movie made angry crappe...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'spoiler': 0.7246482402453214, 'pants': 0.579...</td>\n",
       "      <td>{'spoiler': -0.10844003409147263, 'pants': -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>meltdown opens scene scientists preparing cond...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'meltdown': 0.6610800099231706, 'opens': 0.92...</td>\n",
       "      <td>{'meltdown': -0.5444905161857605, 'opens': 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>spoilers&lt;br /&gt;&lt;br /&gt;this movie action packed; ...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'movie': 0.7419759867804996, 'action': 0.9048...</td>\n",
       "      <td>{'movie': 0.3418169915676117, 'action': 0.0604...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>people say children annoying u think ya little...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'people': 0.9262139591498093, 'say': 0.954317...</td>\n",
       "      <td>{'people': 0.12654943764209747, 'say': 0.04381...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>boooooooorrrrrinngggggggg stooooooopiddddd. ke...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'kept': 0.9474132582509731, 'falling': 0.8058...</td>\n",
       "      <td>{'kept': 0.1641082465648651, 'falling': -0.347...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label    set  \\\n",
       "0      guy desperate action attempts hit gorgeous gir...      1  train   \n",
       "1      ahista ahista one little small brilliant. star...      1  train   \n",
       "2      wracked guilt lot things felt apart ledge, ace...      1  train   \n",
       "3      think hollow point funny film good moments nev...      1  train   \n",
       "4      dana andrews stands \"where sidewalk ends\" 1950...      1  train   \n",
       "...                                                  ...    ...    ...   \n",
       "49995  spoiler pants thinking movie made angry crappe...      0   test   \n",
       "49996  meltdown opens scene scientists preparing cond...      0   test   \n",
       "49997  spoilers<br /><br />this movie action packed; ...      0   test   \n",
       "49998  people say children annoying u think ya little...      0   test   \n",
       "49999  boooooooorrrrrinngggggggg stooooooopiddddd. ke...      0   test   \n",
       "\n",
       "                                              vector_soc  \\\n",
       "0      {'guy': 0.9481098584732381, 'desperate': 0.813...   \n",
       "1      {'little': 0.9747448137394689, 'small': 0.9277...   \n",
       "2      {'wracked': 0.5912880078969615, 'guilt': 0.635...   \n",
       "3      {'think': 0.9601986716024294, 'hollow': 0.7854...   \n",
       "4      {'dana': 0.5221360341507066, 'andrews': 0.5030...   \n",
       "...                                                  ...   \n",
       "49995  {'spoiler': 0.7246482402453214, 'pants': 0.579...   \n",
       "49996  {'meltdown': 0.6610800099231706, 'opens': 0.92...   \n",
       "49997  {'movie': 0.7419759867804996, 'action': 0.9048...   \n",
       "49998  {'people': 0.9262139591498093, 'say': 0.954317...   \n",
       "49999  {'kept': 0.9474132582509731, 'falling': 0.8058...   \n",
       "\n",
       "                                             vector_dens  \n",
       "0      {'guy': 0.14247038960456848, 'desperate': 0.09...  \n",
       "1      {'little': 0.34360048174858093, 'small': 0.604...  \n",
       "2      {'wracked': -0.3791770935058594, 'guilt': 0.01...  \n",
       "3      {'think': 0.16891370713710785, 'hollow': 0.324...  \n",
       "4      {'dana': 0.6259295344352722, 'andrews': 0.6521...  \n",
       "...                                                  ...  \n",
       "49995  {'spoiler': -0.10844003409147263, 'pants': -0....  \n",
       "49996  {'meltdown': -0.5444905161857605, 'opens': 0.4...  \n",
       "49997  {'movie': 0.3418169915676117, 'action': 0.0604...  \n",
       "49998  {'people': 0.12654943764209747, 'say': 0.04381...  \n",
       "49999  {'kept': 0.1641082465648651, 'falling': -0.347...  \n",
       "\n",
       "[50000 rows x 5 columns]"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_reviews = imdb_reviews.apply(create_dict_value, axis=1)\n",
    "imdb_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-workshop",
   "metadata": {},
   "source": [
    "create the vectorizer from the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "stable-vegetable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 58874)\n",
      "(50000, 58874)\n"
     ]
    }
   ],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "v_soc = v.fit_transform(imdb_reviews['vector_soc'])\n",
    "v_dens = v.fit_transform(imdb_reviews['vector_dens'])\n",
    "print(v_soc.shape)\n",
    "print(v_dens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "id": "under-aging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53489828, 0.87679611, 0.75959449, 0.65798654, 0.66686455,\n",
       "       0.6613475 , 0.95913541, 0.95975107, 0.9264372 , 0.85557129, ...,\n",
       "       0.95505506, 0.95713479, 0.97358063, 0.78794464, 0.93159836,\n",
       "       0.94900918, 0.94667836, 0.94449698, 0.84076674, 0.95604063])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (50000, 58874)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold= 10, edgeitems=10)\n",
    "\n",
    "display(v_soc[1][v_soc[1] > 0])\n",
    "print('shape: ', v_soc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-assurance",
   "metadata": {},
   "source": [
    "Fit a logistic classifier with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "synthetic-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_soc = LogisticRegression(random_state=0, max_iter = 8000).fit(v_soc[imdb_reviews['set'] == 'train'], imdb_reviews[imdb_reviews['set'] == 'train']['label'])\n",
    "clf_dens = LogisticRegression(random_state=0, max_iter = 8000).fit(v_dens[imdb_reviews['set'] == 'train'], imdb_reviews[imdb_reviews['set'] == 'train']['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-share",
   "metadata": {},
   "source": [
    "Predict class on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "heated-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_soc = clf_soc.predict(v_soc[imdb_reviews['set'] == 'test'])\n",
    "y_score_dens = clf_dens.predict(v_dens[imdb_reviews['set'] == 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "straight-lincoln",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report soc: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85     12565\n",
      "           1       0.85      0.85      0.85     12435\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.85     25000\n",
      "weighted avg       0.85      0.85      0.85     25000\n",
      "\n",
      "(0.6973694284984095, 0.0)\n",
      "Classification report densify: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85     12497\n",
      "           1       0.85      0.85      0.85     12503\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.85     25000\n",
      "weighted avg       0.85      0.85      0.85     25000\n",
      "\n",
      "(0.7032800202544648, 0.0)\n"
     ]
    }
   ],
   "source": [
    "report_soc = classification_report(y_score_soc, imdb_reviews[imdb_reviews['set'] == 'test']['label'])\n",
    "report_dens = classification_report(y_score_dens, imdb_reviews[imdb_reviews['set'] == 'test']['label'])\n",
    "\n",
    "print('Classification report soc: \\n', report_soc)\n",
    "print(stats.pearsonr(y_score_soc, imdb_reviews[imdb_reviews['set'] == 'test']['label']))\n",
    "print('Classification report densify: \\n',report_dens)\n",
    "print(stats.pearsonr(y_score_dens, imdb_reviews[imdb_reviews['set'] == 'test']['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-mongolia",
   "metadata": {},
   "source": [
    "###### Classification on the content of the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "id": "middle-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys_socialsent = sorted(dict_socialsent, key=dict_socialsent.get)\n",
    "sorted_keys_densify= sorted(dict_densify, key=dict_densify.get)\n",
    "\n",
    "\n",
    "wanted_keys = sorted_keys_socialsent[:3000] + sorted_keys_socialsent[-3000:]\n",
    "dict_socialsent = dict((k, dict_socialsent[k]) for k in wanted_keys)\n",
    "wanted_keys_dens = sorted_keys_densify[:3000] + sorted_keys_densify[-3000:]\n",
    "dict_densify = dict((k, dict_densify[k]) for k in wanted_keys_dens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "first-aspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values: \n",
      " ['disgusting', 'horrible', 'unpleasant', 'horrendous', 'senseless', 'appalling', 'shameful', 'vile', 'disgraceful', 'hideous', 'horrific', 'tragic', 'ghastly', 'unspeakable', 'unfortunate', 'terrible', 'atrocious', 'horrors', 'horrifying', 'sickening', 'dreadful', 'revolting', 'deplorable', 'abominable', 'cowardly', 'terrifying', 'barbaric', 'despicable', 'frightening', 'scandalous', 'barbarous', 'unforgivable', 'odious', 'heinous', 'abhorrent', 'nightmare', 'dastardly', 'reprehensible', 'inexcusable', 'unimaginable', 'shocking', 'horrid', 'grisly', 'repugnant', 'inhuman', 'contemptible', 'grotesque', 'nauseating', 'tragedy', 'gruesome']\n",
      "Positive values: \n",
      " ['blind', 'remembered', 'likes', 'dreams', 'friends', 'beautiful', 'dad', 'best', 'fine', 'choice', 'wise', 'beloved', 'grateful', 'reminds', 'remembers', 'loves', 'life', 'job', 'lot', 'great', \"'m\", 'way', 'easy', 'course', 'plenty', 'better', 'makes', 'simple', 'big', 'always', 'remember', 'little', 'touch', 'liked', 'real', 'fortunate', 'lovely', 'enjoy', 'dream', 'lovers', 'pleasant', 'luck', 'loving', 'nice', 'excellent', 'perfect', 'love', 'happy', 'good', 'loved']\n"
     ]
    }
   ],
   "source": [
    "print('Negative values: \\n', wanted_keys[:50])\n",
    "print('Positive values: \\n', wanted_keys[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "id": "compliant-festival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values: \n",
      " ['lynching', 'usury', 'anti-black', 'beetleborgs', 'anti-catholic', 'ooops', 'discriminates', 'drug-related', 'ouch', 'qld', 'loek', 'violators', 'lynchings', 'puking', 'perpetuated', 'anti-serb', 'vd', 'jayhawkers', 'scrooged', 'wildside', 'thuggery', '1981-82', 'sweatshops', 'pshaw', 'linette', 'dept', 'stereotyping', 'excesses', 'trivialising', 'miscegenation', 'noooo', 'littering', 'ewww', 'sedition', 'rampant', 'afoul', 'sheb', 'profiteering', 'mccarthyism', 'commercialism', 'gouging', 'editorializing', 'blasphemy', 'deplorably', 'overreacting', '______', 'curbed', 'hooliganism', 'feder', 'mongering']\n",
      "Positive values: \n",
      " ['summit', 'accompany', 'tour', 'beautiful', 'elegant', 'alongside', 'deliver', 'gives', 'airy', 'presentation', 'discussions', 'inspiration', 'host', 'quest', 'splendid', 'wonderful', 'chosen', 'pairing', 'arrangements', 'lovely', 'arrival', 'stages', 'distant', 'offering', 'arrive', 'travels', 'partners', 'arrives', 'visit', 'exploring', 'invited', 'grace', 'unique', 'trip', 'arrangement', 'shared', 'offer', 'arrange', 'endeavor', 'offered', 'stage', 'friendship', 'partnership', 'mates', 'complement', 'offers', 'guests', 'explore', 'closest', 'arranged']\n"
     ]
    }
   ],
   "source": [
    "print('Negative values: \\n', wanted_keys_dens[:50])\n",
    "print('Positive values: \\n', wanted_keys_dens[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "elementary-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    evaluation_soc = 0  # socialsent\n",
    "    evaluation_dens = 0   # densify\n",
    "    counter_soc = 0\n",
    "    counter_dens = 0\n",
    "    for word in data['review'].split():\n",
    "        if dict_socialsent.get(word) is not None:\n",
    "            evaluation_soc += dict_socialsent.get(word)\n",
    "            counter_soc += 1\n",
    "        if dict_densify.get(word) is not None:    \n",
    "            evaluation_dens += dict_densify.get(word)\n",
    "            counter_dens += 1\n",
    "    if counter_soc == 0:\n",
    "        counter_soc = 1   # avoid division by 0\n",
    "        counter_dens = 1\n",
    "    data['evaluation_soc_all'] = evaluation_soc/counter_soc\n",
    "    data['evaluation_dens_all'] = evaluation_dens/counter_dens\n",
    "    data['evaluation_soc'] = 1 if evaluation_soc/counter_soc >= 0.82 else 0\n",
    "    data['evaluation_dens'] = 1 if evaluation_dens/counter_dens > 0.25 else 0\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "willing-companion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "      <th>vector_soc</th>\n",
       "      <th>vector_dens</th>\n",
       "      <th>evaluation_soc_all</th>\n",
       "      <th>evaluation_dens_all</th>\n",
       "      <th>evaluation_soc</th>\n",
       "      <th>evaluation_dens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>previously seen short vhs tape feature summer ...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>{'previously': 0.9273066290104699, 'seen': 0.9...</td>\n",
       "      <td>{'previously': 0.2852485775947571, 'seen': 0.2...</td>\n",
       "      <td>0.924398</td>\n",
       "      <td>0.367772</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>one successful shows television history back. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>{'successful': 0.9423904039562421, 'shows': 0....</td>\n",
       "      <td>{'successful': 0.6003767848014832, 'shows': 0....</td>\n",
       "      <td>0.926901</td>\n",
       "      <td>0.343104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>remember film,it first film watched cinema pic...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>{'remember': 0.9745676232808507, 'first': 0.90...</td>\n",
       "      <td>{'remember': 0.13504120707511902, 'first': 0.5...</td>\n",
       "      <td>0.921300</td>\n",
       "      <td>0.266118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>salinger's \"a perfect day bananafish\" ends sui...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>{'perfect': 0.9878818466695282, 'day': 0.90316...</td>\n",
       "      <td>{'perfect': 0.6446346044540405, 'day': 0.39365...</td>\n",
       "      <td>0.909428</td>\n",
       "      <td>0.310720</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>first saw movie video store and, bam margera f...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>{'first': 0.9055403602457166, 'saw': 0.9339838...</td>\n",
       "      <td>{'first': 0.5304829478263855, 'saw': 0.1342318...</td>\n",
       "      <td>0.896786</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>spoiler pants thinking movie made angry crappe...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'spoiler': 0.7246482402453214, 'pants': 0.579...</td>\n",
       "      <td>{'spoiler': -0.10844003409147263, 'pants': -0....</td>\n",
       "      <td>0.896641</td>\n",
       "      <td>0.280523</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>meltdown opens scene scientists preparing cond...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'meltdown': 0.6610800099231706, 'opens': 0.92...</td>\n",
       "      <td>{'meltdown': -0.5444905161857605, 'opens': 0.4...</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>0.283988</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>spoilers&lt;br /&gt;&lt;br /&gt;this movie action packed; ...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'movie': 0.7419759867804996, 'action': 0.9048...</td>\n",
       "      <td>{'movie': 0.3418169915676117, 'action': 0.0604...</td>\n",
       "      <td>0.869731</td>\n",
       "      <td>0.275286</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>people say children annoying u think ya little...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'people': 0.9262139591498093, 'say': 0.954317...</td>\n",
       "      <td>{'people': 0.12654943764209747, 'say': 0.04381...</td>\n",
       "      <td>0.834347</td>\n",
       "      <td>0.248250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>boooooooorrrrrinngggggggg stooooooopiddddd. ke...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'kept': 0.9474132582509731, 'falling': 0.8058...</td>\n",
       "      <td>{'kept': 0.1641082465648651, 'falling': -0.347...</td>\n",
       "      <td>0.927264</td>\n",
       "      <td>0.272835</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label   set  \\\n",
       "25000  previously seen short vhs tape feature summer ...      1  test   \n",
       "25001  one successful shows television history back. ...      1  test   \n",
       "25002  remember film,it first film watched cinema pic...      1  test   \n",
       "25003  salinger's \"a perfect day bananafish\" ends sui...      1  test   \n",
       "25004  first saw movie video store and, bam margera f...      1  test   \n",
       "...                                                  ...    ...   ...   \n",
       "49995  spoiler pants thinking movie made angry crappe...      0  test   \n",
       "49996  meltdown opens scene scientists preparing cond...      0  test   \n",
       "49997  spoilers<br /><br />this movie action packed; ...      0  test   \n",
       "49998  people say children annoying u think ya little...      0  test   \n",
       "49999  boooooooorrrrrinngggggggg stooooooopiddddd. ke...      0  test   \n",
       "\n",
       "                                              vector_soc  \\\n",
       "25000  {'previously': 0.9273066290104699, 'seen': 0.9...   \n",
       "25001  {'successful': 0.9423904039562421, 'shows': 0....   \n",
       "25002  {'remember': 0.9745676232808507, 'first': 0.90...   \n",
       "25003  {'perfect': 0.9878818466695282, 'day': 0.90316...   \n",
       "25004  {'first': 0.9055403602457166, 'saw': 0.9339838...   \n",
       "...                                                  ...   \n",
       "49995  {'spoiler': 0.7246482402453214, 'pants': 0.579...   \n",
       "49996  {'meltdown': 0.6610800099231706, 'opens': 0.92...   \n",
       "49997  {'movie': 0.7419759867804996, 'action': 0.9048...   \n",
       "49998  {'people': 0.9262139591498093, 'say': 0.954317...   \n",
       "49999  {'kept': 0.9474132582509731, 'falling': 0.8058...   \n",
       "\n",
       "                                             vector_dens  evaluation_soc_all  \\\n",
       "25000  {'previously': 0.2852485775947571, 'seen': 0.2...            0.924398   \n",
       "25001  {'successful': 0.6003767848014832, 'shows': 0....            0.926901   \n",
       "25002  {'remember': 0.13504120707511902, 'first': 0.5...            0.921300   \n",
       "25003  {'perfect': 0.6446346044540405, 'day': 0.39365...            0.909428   \n",
       "25004  {'first': 0.5304829478263855, 'saw': 0.1342318...            0.896786   \n",
       "...                                                  ...                 ...   \n",
       "49995  {'spoiler': -0.10844003409147263, 'pants': -0....            0.896641   \n",
       "49996  {'meltdown': -0.5444905161857605, 'opens': 0.4...            0.897780   \n",
       "49997  {'movie': 0.3418169915676117, 'action': 0.0604...            0.869731   \n",
       "49998  {'people': 0.12654943764209747, 'say': 0.04381...            0.834347   \n",
       "49999  {'kept': 0.1641082465648651, 'falling': -0.347...            0.927264   \n",
       "\n",
       "       evaluation_dens_all  evaluation_soc  evaluation_dens  \n",
       "25000             0.367772               1                1  \n",
       "25001             0.343104               1                1  \n",
       "25002             0.266118               1                1  \n",
       "25003             0.310720               1                1  \n",
       "25004             0.226796               1                0  \n",
       "...                    ...             ...              ...  \n",
       "49995             0.280523               1                1  \n",
       "49996             0.283988               1                1  \n",
       "49997             0.275286               1                1  \n",
       "49998             0.248250               1                0  \n",
       "49999             0.272835               1                1  \n",
       "\n",
       "[25000 rows x 9 columns]"
      ]
     },
     "execution_count": 907,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_reviews_evaluated = imdb_reviews[imdb_reviews['set'] == 'test'].apply(evaluate, axis=1)\n",
    "imdb_reviews_evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "occupational-sheffield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    25000.00000\n",
       "mean         0.87548\n",
       "std          0.33018\n",
       "min          0.00000\n",
       "25%          1.00000\n",
       "50%          1.00000\n",
       "75%          1.00000\n",
       "max          1.00000\n",
       "Name: evaluation_dens, dtype: float64"
      ]
     },
     "execution_count": 908,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_reviews_evaluated['evaluation_dens'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-marking",
   "metadata": {},
   "source": [
    "get statts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "id": "naval-joint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson socialsent:  (0.12891610246013305, 4.164848938816207e-93)\n",
      "Pearson densify:  (0.19420072327819932, 5.4140821374103835e-211)\n",
      "Classification report soc: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.81      0.13      1062\n",
      "           1       0.98      0.51      0.67     23938\n",
      "\n",
      "    accuracy                           0.53     25000\n",
      "   macro avg       0.53      0.66      0.40     25000\n",
      "weighted avg       0.94      0.53      0.65     25000\n",
      "\n",
      "Classification report densify: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.76      0.30      3113\n",
      "           1       0.94      0.54      0.68     21887\n",
      "\n",
      "    accuracy                           0.56     25000\n",
      "   macro avg       0.56      0.65      0.49     25000\n",
      "weighted avg       0.85      0.56      0.64     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_socialsent = stats.pearsonr(imdb_reviews_evaluated['evaluation_soc'], imdb_reviews_evaluated['label'])\n",
    "eval_densify = stats.pearsonr(imdb_reviews_evaluated['evaluation_dens'], imdb_reviews_evaluated['label'])\n",
    "print('Pearson socialsent: ', eval_socialsent)\n",
    "print('Pearson densify: ', eval_densify)\n",
    "\n",
    "report_soc = classification_report(imdb_reviews_evaluated['evaluation_soc'], imdb_reviews_evaluated['label'])\n",
    "report_dens = classification_report(imdb_reviews_evaluated['evaluation_dens'], imdb_reviews_evaluated['label'])\n",
    "\n",
    "print('Classification report soc: \\n', report_soc)\n",
    "print('Classification report densify: \\n',report_dens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "id": "antique-praise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sometimes intentionally campy, times unintentionally silly (like opening scene, woman \"informed\" raped family slaughtered, sake exposition), film ultimately neither funny enough competent enough (as straightforward adventure story) really enjoyable. i'll leave decide may highlight, low point probably fight silly metallic dragon. brigitte nielsen good choice sonja, fresh face firm, slightly muscular body (and think dark-red hair suits perfectly), schwarzenegger gives one colorless performances calidor. (*1/2)\n",
      "{'sometimes': 0.9501110285917846, 'intentionally': 0.6920996939485773, 'times': 0.9258533604134269, 'unintentionally': 0.5723260764729114, 'silly': 0.2725723304363115, 'opening': 0.905864061607587, 'woman': 0.8645266773118258, 'raped': 0.6073616131473395, 'family': 0.9007768166762729, 'sake': 0.90133546709098, 'film': 0.7076635137747269, 'ultimately': 0.9508800936015241, 'neither': 0.9558237542679288, 'funny': 0.48189733365390325, 'enough': 0.9594047226864929, 'competent': 0.7025436860378029, 'straightforward': 0.7615487153895225, 'adventure': 0.7652909779920459, 'really': 0.9642968965192684, 'leave': 0.944050845954689, 'decide': 0.934618409310484, 'may': 0.9429760885566294, 'low': 0.8733696560049173, 'point': 0.9473203595961418, 'probably': 0.9555456489250078, 'fight': 0.8490620016810395, 'metallic': 0.7050858402515269, 'brigitte': 0.551630853465507, 'nielsen': 0.6165508935962182, 'good': 0.9887882422521174, 'choice': 0.9694217429892382, 'fresh': 0.8760490296114565, 'slightly': 0.8689513471392015, 'muscular': 0.6251673071499886, 'body': 0.8901722429968183, 'think': 0.9601986716024294, 'hair': 0.6247198270759268, 'suits': 0.6421934454584943, 'schwarzenegger': 0.6679370835468461, 'gives': 0.9606668087713373, 'colorless': 0.5971938036640082, 'performances': 0.8401729731306524}\n",
      "{'sometimes': 0.26928406953811646, 'intentionally': -0.2709348201751709, 'times': 0.026257500052452087, 'unintentionally': -0.29522600769996643, 'silly': -0.25016969442367554, 'opening': 0.5043758153915405, 'woman': 0.29484495520591736, 'raped': -0.38896194100379944, 'family': 0.6549885869026184, 'sake': 0.15870796144008636, 'film': 0.3696841299533844, 'ultimately': 0.24627210199832916, 'neither': 0.4277117848396301, 'funny': 0.2024819254875183, 'enough': 0.35738515853881836, 'competent': 0.24077260494232178, 'straightforward': 0.3997742235660553, 'adventure': 0.5827733278274536, 'really': 0.22413736581802368, 'leave': 0.47032037377357483, 'decide': 0.35949745774269104, 'may': 0.27666518092155457, 'low': 0.071785107254982, 'point': 0.37421852350234985, 'probably': 0.3101063668727875, 'fight': -0.04815303534269333, 'metallic': 0.15965355932712555, 'brigitte': -0.005417530424892902, 'nielsen': -0.0855729728937149, 'good': 0.4529092609882355, 'choice': 0.4721129238605499, 'fresh': 0.2985844016075134, 'slightly': 0.02788703143596649, 'muscular': 0.1663416177034378, 'body': 0.4067302942276001, 'think': 0.16891370713710785, 'hair': 0.1493818759918213, 'suits': 0.021291617304086685, 'schwarzenegger': 0.07399886101484299, 'gives': 0.7628794312477112, 'colorless': -0.08817174285650253, 'performances': 0.4981432259082794}\n"
     ]
    }
   ],
   "source": [
    "print(imdb_reviews_evaluated[24990:24991][['review', 'vector_soc']]['review'][49990])\n",
    "print(imdb_reviews_evaluated[24990:24991][['review', 'vector_soc']]['vector_soc'][49990])\n",
    "print(imdb_reviews_evaluated[24990:24991][['review', 'vector_dens']]['vector_dens'][49990])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "id": "saved-birmingham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "      <th>vector_soc</th>\n",
       "      <th>vector_dens</th>\n",
       "      <th>evaluation_soc_all</th>\n",
       "      <th>evaluation_dens_all</th>\n",
       "      <th>evaluation_soc</th>\n",
       "      <th>evaluation_dens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25179</th>\n",
       "      <td>mighty like moose one many short films directo...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>{'mighty': 0.7312970324769965, 'like': 0.96620...</td>\n",
       "      <td>{'mighty': 0.13502411544322968, 'like': 0.3380...</td>\n",
       "      <td>0.807178</td>\n",
       "      <td>0.336687</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25220</th>\n",
       "      <td>three outstanding bbc television series' chris...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>{'three': 0.6058560404992323, 'outstanding': 0...</td>\n",
       "      <td>{'three': 0.44054365158081055, 'outstanding': ...</td>\n",
       "      <td>0.809603</td>\n",
       "      <td>0.367903</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25230</th>\n",
       "      <td>amazing movie clever using actors sets. also s...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>{'amazing': 0.7880371402648724, 'movie': 0.741...</td>\n",
       "      <td>{'amazing': 0.509014368057251, 'movie': 0.3418...</td>\n",
       "      <td>0.791260</td>\n",
       "      <td>0.296607</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25320</th>\n",
       "      <td>sell dead (2009) **1/2 dominic monaghan, larry...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>{'sell': 0.817990609241611, 'dead': 0.87188900...</td>\n",
       "      <td>{'sell': 0.3423904776573181, 'dead': 0.2134975...</td>\n",
       "      <td>0.747936</td>\n",
       "      <td>0.211122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25353</th>\n",
       "      <td>hunt justice setup slobadon milosevic trial ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>{'hunt': 0.6210317587919527, 'justice': 0.8084...</td>\n",
       "      <td>{'hunt': 0.36053332686424255, 'justice': -0.14...</td>\n",
       "      <td>0.814706</td>\n",
       "      <td>0.279075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49900</th>\n",
       "      <td>oh. good. grief.&lt;br /&gt;&lt;br /&gt;i saw movie title ...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'saw': 0.9339838965670254, 'movie': 0.7419759...</td>\n",
       "      <td>{'saw': 0.13423189520835876, 'movie': 0.341816...</td>\n",
       "      <td>0.797076</td>\n",
       "      <td>0.309812</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49937</th>\n",
       "      <td>los angeles, alcoholic lazy hank chinaski (mat...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'alcoholic': 0.6243188209094036, 'lazy': 0.49...</td>\n",
       "      <td>{'alcoholic': -0.32522428035736084, 'lazy': -0...</td>\n",
       "      <td>0.813735</td>\n",
       "      <td>0.351785</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49944</th>\n",
       "      <td>script, story, mess!</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49945</th>\n",
       "      <td>terrible movie, waste money it. even watch fre...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'terrible': 0.07256369814975122, 'waste': 0.6...</td>\n",
       "      <td>{'terrible': -0.37290361523628235, 'waste': -0...</td>\n",
       "      <td>0.713271</td>\n",
       "      <td>0.216330</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49982</th>\n",
       "      <td>terrible acting, terrible script, wholly unrea...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>{'terrible': 0.07256369814975122, 'wholly': 0....</td>\n",
       "      <td>{'terrible': -0.37290361523628235, 'wholly': 0...</td>\n",
       "      <td>0.644120</td>\n",
       "      <td>0.173147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1062 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label   set  \\\n",
       "25179  mighty like moose one many short films directo...      1  test   \n",
       "25220  three outstanding bbc television series' chris...      1  test   \n",
       "25230  amazing movie clever using actors sets. also s...      1  test   \n",
       "25320  sell dead (2009) **1/2 dominic monaghan, larry...      1  test   \n",
       "25353  hunt justice setup slobadon milosevic trial ha...      1  test   \n",
       "...                                                  ...    ...   ...   \n",
       "49900  oh. good. grief.<br /><br />i saw movie title ...      0  test   \n",
       "49937  los angeles, alcoholic lazy hank chinaski (mat...      0  test   \n",
       "49944                               script, story, mess!      0  test   \n",
       "49945  terrible movie, waste money it. even watch fre...      0  test   \n",
       "49982  terrible acting, terrible script, wholly unrea...      0  test   \n",
       "\n",
       "                                              vector_soc  \\\n",
       "25179  {'mighty': 0.7312970324769965, 'like': 0.96620...   \n",
       "25220  {'three': 0.6058560404992323, 'outstanding': 0...   \n",
       "25230  {'amazing': 0.7880371402648724, 'movie': 0.741...   \n",
       "25320  {'sell': 0.817990609241611, 'dead': 0.87188900...   \n",
       "25353  {'hunt': 0.6210317587919527, 'justice': 0.8084...   \n",
       "...                                                  ...   \n",
       "49900  {'saw': 0.9339838965670254, 'movie': 0.7419759...   \n",
       "49937  {'alcoholic': 0.6243188209094036, 'lazy': 0.49...   \n",
       "49944                                                 {}   \n",
       "49945  {'terrible': 0.07256369814975122, 'waste': 0.6...   \n",
       "49982  {'terrible': 0.07256369814975122, 'wholly': 0....   \n",
       "\n",
       "                                             vector_dens  evaluation_soc_all  \\\n",
       "25179  {'mighty': 0.13502411544322968, 'like': 0.3380...            0.807178   \n",
       "25220  {'three': 0.44054365158081055, 'outstanding': ...            0.809603   \n",
       "25230  {'amazing': 0.509014368057251, 'movie': 0.3418...            0.791260   \n",
       "25320  {'sell': 0.3423904776573181, 'dead': 0.2134975...            0.747936   \n",
       "25353  {'hunt': 0.36053332686424255, 'justice': -0.14...            0.814706   \n",
       "...                                                  ...                 ...   \n",
       "49900  {'saw': 0.13423189520835876, 'movie': 0.341816...            0.797076   \n",
       "49937  {'alcoholic': -0.32522428035736084, 'lazy': -0...            0.813735   \n",
       "49944                                                 {}            0.000000   \n",
       "49945  {'terrible': -0.37290361523628235, 'waste': -0...            0.713271   \n",
       "49982  {'terrible': -0.37290361523628235, 'wholly': 0...            0.644120   \n",
       "\n",
       "       evaluation_dens_all  evaluation_soc  evaluation_dens  \n",
       "25179             0.336687               0                1  \n",
       "25220             0.367903               0                1  \n",
       "25230             0.296607               0                1  \n",
       "25320             0.211122               0                0  \n",
       "25353             0.279075               0                1  \n",
       "...                    ...             ...              ...  \n",
       "49900             0.309812               0                1  \n",
       "49937             0.351785               0                1  \n",
       "49944             0.000000               0                0  \n",
       "49945             0.216330               0                0  \n",
       "49982             0.173147               0                0  \n",
       "\n",
       "[1062 rows x 9 columns]"
      ]
     },
     "execution_count": 911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_reviews_evaluated[imdb_reviews_evaluated['evaluation_soc'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "id": "tribal-paradise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.182567049375833"
      ]
     },
     "execution_count": 945,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#['love', 'loved', 'loves', 'awesome', 'nice', 'amazing', 'best', 'fantastic', 'correct', 'happy']\n",
    "#>>> neg_seeds\n",
    "#['hate', 'hated', 'hates', 'terrible', 'nasty', 'awful', 'worst', 'horrible', 'wrong', 'sad']\n",
    "dict_socialsent.get('sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "burning-proposal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30070081916310654"
      ]
     },
     "execution_count": 919,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(imdb_reviews_evaluated[imdb_reviews_evaluated['label'] == 0]['evaluation_soc_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "id": "worthy-guitar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07578752934932709"
      ]
     },
     "execution_count": 914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(imdb_reviews_evaluated[imdb_reviews_evaluated['label'] == 0]['evaluation_dens_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "id": "alert-dating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4907470460663988"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_socialsent.get(sorted_keys_socialsent[2999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "talented-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sampling'"
      ]
     },
     "execution_count": 916,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_keys_socialsent[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "loving-sending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 921,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
