{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial-terrorism",
   "metadata": {},
   "source": [
    "# IMDB evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "floating-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-voluntary",
   "metadata": {},
   "source": [
    "We are going to create the test dataset. We iterate randdomly through the dataset and take equal number of positive reviews than negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "binary-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = 1000\n",
    "\n",
    "file_list_pos = random.sample(os.listdir('aclImdb/test/pos'), n_files)  # select x random files\n",
    "file_list_neg = random.sample(os.listdir('aclImdb/test/neg'), n_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-edwards",
   "metadata": {},
   "source": [
    "Create the groundtruth lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cognitive-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_gt = np.append(np.ones(n_files),np.ones(n_files)*-1)  # ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-harrison",
   "metadata": {},
   "source": [
    "get the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "promising-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionaries/imdb_densify.json') as json_file:\n",
    "    dict_densify = json.load(json_file)\n",
    "with open('dictionaries/imdb_socialsent.json') as json_file:\n",
    "    dict_socialsent = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-healthcare",
   "metadata": {},
   "source": [
    "Function to normalize the dictionary between -1 and 1 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "sharing-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dict(dictionary):\n",
    "    list(dict_densify.values())\n",
    "\n",
    "    value_list = list(dictionary.values())\n",
    "    key_list = list(dictionary.keys())\n",
    "    max_list = max(value_list)\n",
    "    min_list = min(value_list)\n",
    "    a = {key_list[i]: 2 * (value_list[i] - min_list)/(max_list - min_list) -1  for i in range(len(key_list))}\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "worthy-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_socialsent = normalize_dict(dict_socialsent)\n",
    "dict_densify = normalize_dict(dict_densify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-assurance",
   "metadata": {},
   "source": [
    "Read each test positive file and calculate the mean value for both densify and socialsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "stopped-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pos_socialsent = np.zeros(n_files)\n",
    "list_pos_densify = np.zeros(n_files)\n",
    "stop_words = set(stopwords.words())   # convert into a set for speed purposes\n",
    "i = 0\n",
    "for file in file_list_pos:\n",
    "    with io.open('aclImdb/test/pos/' + file, 'r', encoding='utf8') as review_file:\n",
    "        review = review_file.read() # read the file\n",
    "    words = review.split()\n",
    "    sum_file_socialsent = 0\n",
    "    sum_file_densify = 0\n",
    "    counter_socialsent = 0\n",
    "    counter_densify = 0\n",
    "    for word in words:\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            if dict_socialsent.get(word) is not None:\n",
    "                sum_file_socialsent += dict_socialsent.get(word)\n",
    "                counter_socialsent += 1\n",
    "            if dict_densify.get(word) is not None:    \n",
    "                sum_file_densify += dict_densify.get(word)\n",
    "                counter_densify += 1\n",
    "    list_pos_socialsent[i] = sum_file_socialsent / counter_socialsent\n",
    "    list_pos_densify[i] = sum_file_densify / counter_densify\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-insured",
   "metadata": {},
   "source": [
    "same for negative files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "internal-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_neg_socialsent = np.zeros(n_files)\n",
    "list_neg_densify = np.zeros(n_files)\n",
    "stop_words = set(stopwords.words())   # convert into a set for speed purposes\n",
    "i = 0\n",
    "for file in file_list_neg:\n",
    "    with io.open('aclImdb/test/neg/' + file, 'r', encoding='utf8') as review_file:\n",
    "        review = review_file.read() # read the file\n",
    "    words = review.split()\n",
    "    sum_file_socialsent = 0\n",
    "    sum_file_densify = 0\n",
    "    counter_socialsent = 0\n",
    "    counter_densify = 0\n",
    "    for word in words:\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            if dict_socialsent.get(word) is not None:\n",
    "                sum_file_socialsent += dict_socialsent.get(word)\n",
    "                counter_socialsent += 1\n",
    "            if dict_densify.get(word) is not None:    \n",
    "                sum_file_densify += dict_densify.get(word)\n",
    "                counter_densify += 1\n",
    "    list_neg_socialsent[i] = sum_file_socialsent / counter_socialsent\n",
    "    list_neg_densify[i] = sum_file_densify / counter_densify\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-activity",
   "metadata": {},
   "source": [
    "Join the positive and negative lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "linear-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_socialsent = np.append(list_pos_socialsent, list_neg_socialsent)\n",
    "list_densify = np.append(list_pos_densify, list_neg_densify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-preference",
   "metadata": {},
   "source": [
    "Evaluate with pearson coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "different-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_socialsent = stats.pearsonr(list_gt, list_socialsent)\n",
    "evaluation_densify = stats.pearsonr(list_gt, list_densify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "endless-chain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socialsent:  (0.30841808455197156, 2.4773227600012926e-45)\n",
      "Densify:  (0.38319927197387577, 5.827877221460614e-71)\n"
     ]
    }
   ],
   "source": [
    "print('Socialsent: ', evaluation_socialsent)\n",
    "print('Densify: ', evaluation_densify)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
