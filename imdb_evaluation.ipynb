{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create the test dataset. We iterate randdomly through the dataset and take equal number of positive reviews than negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = 1000\n",
    "\n",
    "file_list_pos = random.sample(os.listdir('aclImdb/test/pos'), n_files)  # select x random files\n",
    "file_list_neg = random.sample(os.listdir('aclImdb/test/neg'), n_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the groundtruth lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_gt = np.append(np.ones(n_files),np.ones(n_files)*-1)  # ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionaries/imdb_shuffle_densify.json') as json_file:\n",
    "    dict_densify = json.load(json_file)\n",
    "with open('dictionaries/imdb_shuffle_socialsent.json') as json_file:\n",
    "    dict_socialsent = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0215262174606323"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(list(dict_densify.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to normalize the dictionary between -1 and 1 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_dict(dictionary):\n",
    "\n",
    "#     value_list = list(dictionary.values())\n",
    "#     key_list = list(dictionary.keys())\n",
    "#     max_list = max(value_list)\n",
    "#     min_list = min(value_list)\n",
    "#     a = {key_list[i]: 2 * (value_list[i] - min_list)/(max_list - min_list) -1  for i in range(len(key_list))}\n",
    "#     return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_socialsent = normalize_dict(dict_socialsent)\n",
    "#dict_densify = normalize_dict(dict_densify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read each test positive file and calculate the mean value for both densify and socialsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pos_socialsent = np.zeros(n_files)\n",
    "list_pos_densify = np.zeros(n_files)\n",
    "stop_words = set(stopwords.words())   # convert into a set for speed purposes\n",
    "i = 0\n",
    "for file in file_list_pos:\n",
    "    with io.open('aclImdb/test/pos/' + file, 'r', encoding='utf8') as review_file:\n",
    "        review = review_file.read() # read the file\n",
    "    words = review.split()\n",
    "    sum_file_socialsent = 0\n",
    "    sum_file_densify = 0\n",
    "    counter_socialsent = 0\n",
    "    counter_densify = 0\n",
    "    for word in words:\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            if dict_socialsent.get(word) is not None:\n",
    "                sum_file_socialsent += dict_socialsent.get(word)\n",
    "                counter_socialsent += 1\n",
    "            if dict_densify.get(word) is not None:    \n",
    "                sum_file_densify += dict_densify.get(word)\n",
    "                counter_densify += 1\n",
    "    list_pos_socialsent[i] = 1 if sum_file_socialsent / counter_socialsent >= 0.5 else -1\n",
    "    list_pos_densify[i] = 1 if sum_file_densify / counter_densify > 0 else -1\n",
    "    i += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same for negative files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_neg_socialsent = np.zeros(n_files)\n",
    "list_neg_densify = np.zeros(n_files)\n",
    "stop_words = set(stopwords.words())   # convert into a set for speed purposes\n",
    "i = 0\n",
    "for file in file_list_neg:\n",
    "    with io.open('aclImdb/test/neg/' + file, 'r', encoding='utf8') as review_file:\n",
    "        review = review_file.read() # read the file\n",
    "    words = review.split()\n",
    "    sum_file_socialsent = 0\n",
    "    sum_file_densify = 0\n",
    "    counter_socialsent = 0\n",
    "    counter_densify = 0\n",
    "    for word in words:\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            if dict_socialsent.get(word) is not None:\n",
    "                sum_file_socialsent += dict_socialsent.get(word)\n",
    "                counter_socialsent += 1\n",
    "            if dict_densify.get(word) is not None:    \n",
    "                sum_file_densify += dict_densify.get(word)\n",
    "                counter_densify += 1\n",
    "    list_neg_socialsent[i] = sum_file_socialsent / counter_socialsent\n",
    "    list_neg_densify[i] = sum_file_densify / counter_densify\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the positive and negative lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_socialsent = np.append(list_pos_socialsent, list_neg_socialsent)\n",
    "list_densify = np.append(list_pos_densify, list_neg_densify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate with pearson coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_socialsent = stats.pearsonr(list_gt, list_socialsent)\n",
    "evaluation_densify = stats.pearsonr(list_gt, list_densify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socialsent:  (0.9938703851511883, 0.0)\n",
      "Densify:  (0.9892299984151911, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print('Socialsent: ', evaluation_socialsent)\n",
    "print('Densify: ', evaluation_densify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40526596495164907"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(list_socialsent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
